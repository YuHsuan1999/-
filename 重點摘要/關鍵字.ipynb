{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ckip-transformers in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (0.2.7)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from ckip-transformers) (4.42.1)\n",
      "Requirement already satisfied: torch>=1.5.0 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from ckip-transformers) (1.9.1)\n",
      "Requirement already satisfied: transformers>=3.5.0 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from ckip-transformers) (4.11.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from torch>=1.5.0->ckip-transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from transformers>=3.5.0->ckip-transformers) (5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from transformers>=3.5.0->ckip-transformers) (0.0.17)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from transformers>=3.5.0->ckip-transformers) (2021.9.24)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from transformers>=3.5.0->ckip-transformers) (0.10.3)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from transformers>=3.5.0->ckip-transformers) (0.0.46)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from transformers>=3.5.0->ckip-transformers) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from transformers>=3.5.0->ckip-transformers) (1.18.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from transformers>=3.5.0->ckip-transformers) (2.22.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from transformers>=3.5.0->ckip-transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from transformers>=3.5.0->ckip-transformers) (21.0)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from sacremoses->transformers>=3.5.0->ckip-transformers) (1.14.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from sacremoses->transformers>=3.5.0->ckip-transformers) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from sacremoses->transformers>=3.5.0->ckip-transformers) (8.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=3.5.0->ckip-transformers) (2.2.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from requests->transformers>=3.5.0->ckip-transformers) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from requests->transformers>=3.5.0->ckip-transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from requests->transformers>=3.5.0->ckip-transformers) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from requests->transformers>=3.5.0->ckip-transformers) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from packaging>=20.0->transformers>=3.5.0->ckip-transformers) (2.4.7)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\user\\anaconda3\\envs\\bert_edmodel\\lib\\site-packages (from click->sacremoses->transformers>=3.5.0->ckip-transformers) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ckip-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
    "\n",
    "#工具分為三個等級（1—3）。等級一最快，等級三（預設值）最精準。\n",
    "ws_driver  = CkipWordSegmenter(level=3)\n",
    "pos_driver = CkipPosTagger(level=3)\n",
    "ner_driver = CkipNerChunker(level=3)\n",
    "\n",
    "#可於宣告斷詞等工具時指定 device 以使用 GPU，設為 -1 （預設值）代表不使用 GPU。\n",
    "ws_driver = CkipWordSegmenter(device=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "<class 'dict'>\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618174\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='台灣職籃聯盟', ner='ORG', idx=(0, 6))\n",
      "NerToken(word='新北國王隊', ner='ORG', idx=(16, 21))\n",
      "NerToken(word='新莊宏匯廣場', ner='FAC', idx=(24, 30))\n",
      "NerToken(word='楊敬敏', ner='PERSON', idx=(52, 55))\n",
      "NerToken(word='聯盟', ner='ORG', idx=(62, 64))\n",
      "NerToken(word='P. LEAGUE', ner='ORG', idx=(88, 97))\n",
      "NerToken(word='PLG', ner='ORG', idx=(101, 104))\n",
      "NerToken(word='阿美族', ner='NORP', idx=(148, 151))\n",
      "NerToken(word='楊敬敏', ner='PERSON', idx=(154, 157))\n",
      "NerToken(word='戴維斯', ner='PERSON', idx=(172, 175))\n",
      "NerToken(word='台法', ner='GPE', idx=(190, 192))\n",
      "NerToken(word='張文平', ner='PERSON', idx=(196, 199))\n",
      "NerToken(word='裕隆納智捷', ner='GPE', idx=(202, 207))\n",
      "NerToken(word='李愷諺', ner='PERSON', idx=(211, 214))\n",
      "NerToken(word='楊敬敏', ner='PERSON', idx=(216, 219))\n",
      "NerToken(word='聯盟', ner='ORG', idx=(232, 234))\n",
      "NerToken(word='台北', ner='GPE', idx=(249, 251))\n",
      "NerToken(word='桃園', ner='GPE', idx=(256, 258))\n",
      "NerToken(word='戴維斯', ner='PERSON', idx=(311, 314))\n",
      "NerToken(word='楊敬敏', ner='PERSON', idx=(315, 318))\n",
      "NerToken(word='戴維斯', ner='PERSON', idx=(339, 342))\n",
      "NerToken(word='超級籃球聯賽', ner='EVENT', idx=(392, 398))\n",
      "NerToken(word='SBL', ner='ORG', idx=(399, 402))\n",
      "NerToken(word='簡祐哲', ner='PERSON', idx=(406, 409))\n",
      "NerToken(word='國王隊', ner='ORG', idx=(415, 418))\n",
      "NerToken(word='簡祐哲', ner='PERSON', idx=(471, 474))\n",
      "NerToken(word='中央社', ner='ORG', idx=(568, 571))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618562\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='中醫師', ner='NORP', idx=(22, 25))\n",
      "NerToken(word='許淳彰', ner='PERSON', idx=(25, 28))\n",
      "NerToken(word='衛福部國民健康署', ner='ORG', idx=(33, 41))\n",
      "NerToken(word='許淳彰', ner='PERSON', idx=(142, 145))\n",
      "NerToken(word='許淳彰', ner='PERSON', idx=(333, 336))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618568\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='新北國王隊', ner='ORG', idx=(14, 19))\n",
      "NerToken(word='台灣人', ner='NORP', idx=(34, 37))\n",
      "NerToken(word='戴維斯', ner='PERSON', idx=(37, 40))\n",
      "NerToken(word='阿美', ner='NORP', idx=(43, 45))\n",
      "NerToken(word='楊敬敏', ner='PERSON', idx=(47, 50))\n",
      "NerToken(word='新北', ner='GPE', idx=(77, 79))\n",
      "NerToken(word='P.LEAGUE', ner='ORG', idx=(112, 120))\n",
      "NerToken(word='桃園', ner='GPE', idx=(141, 143))\n",
      "NerToken(word='新竹', ner='GPE', idx=(146, 148))\n",
      "NerToken(word='高雄', ner='GPE', idx=(166, 168))\n",
      "NerToken(word='新北國王隊', ner='ORG', idx=(179, 184))\n",
      "NerToken(word='新北國王隊', ner='ORG', idx=(194, 199))\n",
      "NerToken(word='毛加恩', ner='PERSON', idx=(202, 205))\n",
      "NerToken(word='楊敬敏', ner='PERSON', idx=(220, 223))\n",
      "NerToken(word='阿美', ner='NORP', idx=(267, 269))\n",
      "NerToken(word='楊敬敏', ner='PERSON', idx=(271, 274))\n",
      "NerToken(word='台灣', ner='GPE', idx=(277, 279))\n",
      "NerToken(word='戴維斯', ner='PERSON', idx=(280, 283))\n",
      "NerToken(word='洪志善', ner='PERSON', idx=(285, 288))\n",
      "NerToken(word='中華隊', ner='ORG', idx=(293, 296))\n",
      "NerToken(word='老中青', ner='PERSON', idx=(301, 304))\n",
      "NerToken(word='新北', ner='GPE', idx=(324, 326))\n",
      "NerToken(word='新北國王隊', ner='ORG', idx=(327, 332))\n",
      "NerToken(word='陳信生', ner='PERSON', idx=(335, 338))\n",
      "NerToken(word='新北市', ner='GPE', idx=(344, 347))\n",
      "NerToken(word='新莊體育館', ner='FAC', idx=(426, 431))\n",
      "NerToken(word='台北富邦', ner='ORG', idx=(444, 448))\n",
      "NerToken(word='新北', ner='PERSON', idx=(451, 453))\n",
      "NerToken(word='新北國王隊', ner='ORG', idx=(470, 475))\n",
      "NerToken(word='張文平', ner='PERSON', idx=(477, 480))\n",
      "NerToken(word='台法', ner='NORP', idx=(552, 554))\n",
      "NerToken(word='張文平', ner='PERSON', idx=(560, 563))\n",
      "NerToken(word='台灣', ner='GPE', idx=(570, 572))\n",
      "NerToken(word='新北', ner='NORP', idx=(586, 588))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618614\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  2.00s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='食品暨藥物管理局', ner='ORG', idx=(30, 38))\n",
      "NerToken(word='FDA', ner='ORG', idx=(39, 42))\n",
      "NerToken(word='輝瑞', ner='ORG', idx=(54, 56))\n",
      "NerToken(word='疾病管制暨預防中心', ner='ORG', idx=(156, 165))\n",
      "NerToken(word=' CDC）', ner='ORG', idx=(209, 214))\n",
      "NerToken(word='寇恩（Amanda Cohn）', ner='PERSON', idx=(216, 231))\n",
      "NerToken(word='美國', ner='GPE', idx=(303, 305))\n",
      "NerToken(word='中央社', ner='ORG', idx=(420, 423))\n",
      "NerToken(word='《TVBS》', ner='WORK_OF_ART', idx=(424, 430))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(436, 440))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(443, 446))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(479, 491))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618632\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.53s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.62s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.39s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.54s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 500.87it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 500.99it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.55s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='內湖國泰診所', ner='ORG', idx=(107, 113))\n",
      "NerToken(word='蔡明珠', ner='PERSON', idx=(120, 123))\n",
      "NerToken(word='台灣', ner='GPE', idx=(426, 428))\n",
      "NerToken(word='蔡明珠', ner='PERSON', idx=(693, 696))\n",
      "NerToken(word='蔡明珠', ner='PERSON', idx=(978, 981))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(1151, 1156))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618652\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.13s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.99s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.01s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.59it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.39s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='台大家醫科', ner='ORG', idx=(76, 81))\n",
      "NerToken(word='何忠祐', ner='PERSON', idx=(83, 86))\n",
      "NerToken(word='何忠祐', ner='PERSON', idx=(116, 119))\n",
      "NerToken(word='mRNA', ner='PRODUCT', idx=(143, 147))\n",
      "NerToken(word='何忠祐', ner='PERSON', idx=(327, 330))\n",
      "NerToken(word='英國', ner='GPE', idx=(479, 481))\n",
      "NerToken(word='Com-COV', ner='PRODUCT', idx=(481, 488))\n",
      "NerToken(word='AZ+BNT', ner='PRODUCT', idx=(495, 501))\n",
      "NerToken(word='AZ+AZ', ner='PRODUCT', idx=(513, 518))\n",
      "NerToken(word='AZ+BNT', ner='PRODUCT', idx=(542, 548))\n",
      "NerToken(word='英國', ner='GPE', idx=(559, 561))\n",
      "NerToken(word='南非', ner='GPE', idx=(562, 564))\n",
      "NerToken(word='巴西', ner='GPE', idx=(565, 567))\n",
      "NerToken(word='AZ+AZ', ner='PRODUCT', idx=(572, 577))\n",
      "NerToken(word='英國', ner='NORP', idx=(588, 590))\n",
      "NerToken(word='南非', ner='GPE', idx=(597, 599))\n",
      "NerToken(word='巴西', ner='GPE', idx=(600, 602))\n",
      "NerToken(word='莫德納', ner='PERSON', idx=(670, 673))\n",
      "NerToken(word='莫德納', ner='PERSON', idx=(696, 699))\n",
      "NerToken(word='何忠祐', ner='PERSON', idx=(766, 769))\n",
      "NerToken(word='莫德納', ner='PERSON', idx=(776, 779))\n",
      "NerToken(word='AZ + AZ', ner='PRODUCT', idx=(869, 876))\n",
      "NerToken(word='南非', ner='GPE', idx=(932, 934))\n",
      "NerToken(word='AZ+AZ', ner='PRODUCT', idx=(945, 950))\n",
      "NerToken(word='南非', ner='GPE', idx=(959, 961))\n",
      "NerToken(word='莫德納', ner='PERSON', idx=(973, 976))\n",
      "NerToken(word='何忠祐', ner='PERSON', idx=(1029, 1032))\n",
      "NerToken(word='英國', ner='GPE', idx=(1047, 1049))\n",
      "NerToken(word='Com-COV', ner='ORG', idx=(1049, 1056))\n",
      "NerToken(word='BNT + AZ', ner='PRODUCT', idx=(1089, 1097))\n",
      "NerToken(word='BNT ', ner='PRODUCT', idx=(1098, 1102))\n",
      "NerToken(word=' BNT', ner='PRODUCT', idx=(1103, 1107))\n",
      "NerToken(word='加拿大', ner='GPE', idx=(1110, 1113))\n",
      "NerToken(word='mRNA', ner='PRODUCT', idx=(1142, 1146))\n",
      "NerToken(word='何忠祐', ner='PERSON', idx=(1202, 1205))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NerToken(word='何忠祐', ner='PERSON', idx=(1320, 1323))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(1384, 1389))\n",
      "NerToken(word='《TVBS》', ner='WORK_OF_ART', idx=(1405, 1411))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(1417, 1421))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(1424, 1427))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(1460, 1472))\n",
      "NerToken(word='HOT', ner='ORG', idx=(1474, 1477))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618667\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.01s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.72s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.95s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.51s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='葉', ner='PERSON', idx=(4, 5))\n",
      "NerToken(word='衛福部立豐原醫院', ner='ORG', idx=(156, 164))\n",
      "NerToken(word='張憲伯', ner='PERSON', idx=(169, 172))\n",
      "NerToken(word='歐', ner='LOC', idx=(217, 218))\n",
      "NerToken(word='美', ner='LOC', idx=(218, 219))\n",
      "NerToken(word='張憲伯', ner='PERSON', idx=(385, 388))\n",
      "NerToken(word='衛福部立豐原醫院', ner='ORG', idx=(576, 584))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(627, 632))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618792\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.98s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.44s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.39s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='日本', ner='NORP', idx=(0, 2))\n",
      "NerToken(word='大谷翔平', ner='PERSON', idx=(4, 8))\n",
      "NerToken(word='美國職棒大聯盟', ner='ORG', idx=(17, 24))\n",
      "NerToken(word='主席歷史成就獎', ner='WORK_OF_ART', idx=(36, 43))\n",
      "NerToken(word='大谷翔平', ner='PERSON', idx=(70, 74))\n",
      "NerToken(word='日本', ner='GPE', idx=(85, 87))\n",
      "NerToken(word='時事通信社', ner='ORG', idx=(87, 92))\n",
      "NerToken(word='洛杉磯', ner='NORP', idx=(106, 109))\n",
      "NerToken(word='大谷翔平', ner='PERSON', idx=(132, 136))\n",
      "NerToken(word='MLB', ner='ORG', idx=(141, 144))\n",
      "NerToken(word='曼佛瑞德', ner='PERSON', idx=(146, 150))\n",
      "NerToken(word='Rob Manfred', ner='PERSON', idx=(151, 162))\n",
      "NerToken(word='主席歷史成就獎', ner='WORK_OF_ART', idx=(166, 173))\n",
      "NerToken(word='主席歷史成就獎', ner='WORK_OF_ART', idx=(220, 227))\n",
      "NerToken(word='大谷', ner='LOC', idx=(269, 271))\n",
      "NerToken(word='大谷翔平', ner='PERSON', idx=(288, 292))\n",
      "NerToken(word='大聯盟', ner='ORG', idx=(350, 353))\n",
      "NerToken(word='日籍', ner='NORP', idx=(353, 355))\n",
      "NerToken(word='鈴木一朗', ner='PERSON', idx=(379, 383))\n",
      "NerToken(word='主席歷史成就獎', ner='WORK_OF_ART', idx=(394, 401))\n",
      "NerToken(word='大谷', ner='PERSON', idx=(402, 404))\n",
      "NerToken(word='日本人', ner='NORP', idx=(413, 416))\n",
      "NerToken(word='大谷', ner='PERSON', idx=(417, 419))\n",
      "NerToken(word='曼佛瑞德', ner='PERSON', idx=(444, 448))\n",
      "NerToken(word='大谷', ner='GPE', idx=(451, 453))\n",
      "NerToken(word='中央社', ner='ORG', idx=(475, 478))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618832\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='二刀流', ner='WORK_OF_ART', idx=(1, 4))\n",
      "NerToken(word='大谷翔平', ner='PERSON', idx=(5, 9))\n",
      "NerToken(word='大聯盟', ner='ORG', idx=(18, 21))\n",
      "NerToken(word='主席歷史成就獎', ner='WORK_OF_ART', idx=(23, 30))\n",
      "NerToken(word='日本籍', ner='NORP', idx=(100, 103))\n",
      "NerToken(word='鈴木一朗', ner='PERSON', idx=(104, 108))\n",
      "NerToken(word='大谷', ner='PERSON', idx=(134, 136))\n",
      "NerToken(word='日籍', ner='NORP', idx=(146, 148))\n",
      "NerToken(word='美國', ner='GPE', idx=(154, 156))\n",
      "NerToken(word='德克薩斯州', ner='GPE', idx=(156, 161))\n",
      "NerToken(word='世界大賽', ner='EVENT', idx=(162, 166))\n",
      "NerToken(word='大聯盟', ner='ORG', idx=(171, 174))\n",
      "NerToken(word='曼佛瑞（Rob Manfred）', ner='PERSON', idx=(176, 192))\n",
      "NerToken(word='大谷翔平', ner='PERSON', idx=(197, 201))\n",
      "NerToken(word='大谷', ner='PERSON', idx=(202, 204))\n",
      "NerToken(word='大聯盟', ner='ORG', idx=(269, 272))\n",
      "NerToken(word='洛杉磯', ner='NORP', idx=(276, 279))\n",
      "NerToken(word='大谷', ner='PERSON', idx=(299, 301))\n",
      "NerToken(word='主席歷史成就獎', ner='WORK_OF_ART', idx=(359, 366))\n",
      "NerToken(word='大谷', ner='PERSON', idx=(421, 423))\n",
      "NerToken(word='美聯明星隊', ner='ORG', idx=(446, 451))\n",
      "NerToken(word='道奇隊', ner='ORG', idx=(527, 530))\n",
      "NerToken(word='史考利', ner='PERSON', idx=(535, 538))\n",
      "NerToken(word='Vin Scully', ner='PERSON', idx=(539, 549))\n",
      "NerToken(word='主席歷史成就獎', ner='WORK_OF_ART', idx=(563, 570))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618846\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='歐盟', ner='ORG', idx=(0, 2))\n",
      "NerToken(word='莫德納', ner='EVENT', idx=(6, 9))\n",
      "NerToken(word='美國FDA', ner='ORG', idx=(45, 50))\n",
      "NerToken(word='輝瑞BNT', ner='PRODUCT', idx=(77, 82))\n",
      "NerToken(word='\\nNBA籃網隊', ner='ORG', idx=(113, 120))\n",
      "NerToken(word='Kyrie Irving', ner='PERSON', idx=(121, 133))\n",
      "NerToken(word='德國', ner='GPE', idx=(170, 172))\n",
      "NerToken(word='慕尼黑', ner='GPE', idx=(174, 177))\n",
      "NerToken(word='德國', ner='GPE', idx=(178, 180))\n",
      "NerToken(word='基米希', ner='PERSON', idx=(182, 185))\n",
      "NerToken(word='德國', ner='GPE', idx=(198, 200))\n",
      "NerToken(word='拜仁', ner='ORG', idx=(200, 202))\n",
      "NerToken(word='慕尼黑', ner='GPE', idx=(202, 205))\n",
      "NerToken(word='基米希', ner='PERSON', idx=(207, 210))\n",
      "NerToken(word='德國', ner='GPE', idx=(239, 241))\n",
      "NerToken(word='歐洲藥物管理局', ner='ORG', idx=(276, 283))\n",
      "NerToken(word='莫德納', ner='PERSON', idx=(288, 291))\n",
      "NerToken(word='美國', ner='GPE', idx=(321, 323))\n",
      "NerToken(word='Irving', ner='PERSON', idx=(326, 332))\n",
      "NerToken(word='美國', ner='GPE', idx=(339, 341))\n",
      "NerToken(word='梅威瑟', ner='PERSON', idx=(344, 347))\n",
      "NerToken(word='美國', ner='GPE', idx=(349, 351))\n",
      "NerToken(word='Mayweather', ner='PERSON', idx=(399, 409))\n",
      "NerToken(word='美國', ner='GPE', idx=(424, 426))\n",
      "NerToken(word='阿拉巴馬州大學兒童感染科', ner='ORG', idx=(445, 457))\n",
      "NerToken(word='金博立', ner='PERSON', idx=(459, 462))\n",
      "NerToken(word='FDA', ner='ORG', idx=(491, 494))\n",
      "NerToken(word='泰國', ner='GPE', idx=(605, 607))\n",
      "NerToken(word='英國', ner='GPE', idx=(628, 630))\n",
      "NerToken(word='AY4.2', ner='PRODUCT', idx=(631, 636))\n",
      "NerToken(word='紐西蘭', ner='GPE', idx=(641, 644))\n",
      "NerToken(word='俄羅斯', ner='GPE', idx=(664, 667))\n",
      "NerToken(word='TVBS', ner='WORK_OF_ART', idx=(699, 703))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(710, 714))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(717, 720))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(753, 765))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618905\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.44s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.66s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.47s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.74s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.91s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.03s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='莫頓', ner='PERSON', idx=(51, 53))\n",
      "NerToken(word='美國職棒大聯盟MLB', ner='ORG', idx=(79, 89))\n",
      "NerToken(word='亞特蘭大', ner='NORP', idx=(111, 115))\n",
      "NerToken(word='休士頓', ner='GPE', idx=(122, 125))\n",
      "NerToken(word='莫頓', ner='PERSON', idx=(145, 147))\n",
      "NerToken(word='Charlie Morton', ner='PERSON', idx=(148, 162))\n",
      "NerToken(word='美國聯盟冠軍賽', ner='EVENT', idx=(171, 178))\n",
      "NerToken(word='巴爾德斯', ner='PERSON', idx=(190, 194))\n",
      "NerToken(word='索萊爾', ner='PERSON', idx=(231, 234))\n",
      "NerToken(word='巴爾德斯', ner='PERSON', idx=(255, 259))\n",
      "NerToken(word='萊里（Austin Riley）', ner='PERSON', idx=(370, 386))\n",
      "NerToken(word='羅沙里歐（Eddie Rosario）', ner='FAC', idx=(420, 439))\n",
      "NerToken(word='杜瓦（Adam Duvall）', ner='PERSON', idx=(445, 460))\n",
      "NerToken(word='巴爾德斯', ner='PERSON', idx=(490, 494))\n",
      "NerToken(word='莫頓', ner='PERSON', idx=(545, 547))\n",
      "NerToken(word='MLB', ner='PRODUCT', idx=(593, 596))\n",
      "NerToken(word='莫頓', ner='PERSON', idx=(610, 612))\n",
      "NerToken(word='莫頓', ner='PERSON', idx=(634, 636))\n",
      "NerToken(word='4下1', ner='EVENT', idx=(641, 644))\n",
      "NerToken(word='塔克（Kyle Tucker）', ner='PERSON', idx=(648, 663))\n",
      "NerToken(word='古利爾', ner='PERSON', idx=(672, 675))\n",
      "NerToken(word='麥克考米克', ner='PERSON', idx=(700, 705))\n",
      "NerToken(word='阿爾瓦雷茲', ner='PERSON', idx=(767, 772))\n",
      "NerToken(word='莫頓', ner='PERSON', idx=(860, 862))\n",
      "NerToken(word='傑克森', ner='PERSON', idx=(908, 911))\n",
      "NerToken(word='馬切克', ner='PERSON', idx=(926, 929))\n",
      "NerToken(word='史密斯', ner='PERSON', idx=(944, 947))\n",
      "NerToken(word='佛里特（Max Fried）', ner='PERSON', idx=(1009, 1023))\n",
      "NerToken(word='厄基迪', ner='PERSON', idx=(1046, 1049))\n",
      "NerToken(word='中央社', ner='ORG', idx=(1067, 1070))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618910\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國職棒大聯盟', ner='ORG', idx=(0, 7))\n",
      "NerToken(word='日籍', ner='NORP', idx=(7, 9))\n",
      "NerToken(word='大谷翔平', ner='PERSON', idx=(12, 16))\n",
      "NerToken(word='天使隊', ner='ORG', idx=(51, 54))\n",
      "NerToken(word='\\nMLB電視網', ner='ORG', idx=(97, 104))\n",
      "NerToken(word='尼爾森', ner='PERSON', idx=(107, 110))\n",
      "NerToken(word='日本', ner='NORP', idx=(112, 114))\n",
      "NerToken(word='MLB', ner='ORG', idx=(125, 128))\n",
      "NerToken(word='曼佛瑞德', ner='PERSON', idx=(130, 134))\n",
      "NerToken(word='大谷', ner='GPE', idx=(137, 139))\n",
      "NerToken(word='日文', ner='LANGUAGE', idx=(158, 160))\n",
      "NerToken(word='大谷翔平', ner='PERSON', idx=(163, 167))\n",
      "NerToken(word='美國職棒大聯盟', ner='ORG', idx=(169, 176))\n",
      "NerToken(word='歷史成就獎', ner='WORK_OF_ART', idx=(183, 188))\n",
      "NerToken(word='日籍', ner='NORP', idx=(192, 194))\n",
      "NerToken(word='大谷翔平', ner='PERSON', idx=(196, 200))\n",
      "NerToken(word='MLB', ner='PRODUCT', idx=(231, 234))\n",
      "NerToken(word='大聯盟世界大賽', ner='EVENT', idx=(253, 260))\n",
      "NerToken(word='大谷', ner='PERSON', idx=(265, 267))\n",
      "NerToken(word='大聯盟', ner='ORG', idx=(271, 274))\n",
      "NerToken(word='曼佛瑞德', ner='PERSON', idx=(276, 280))\n",
      "NerToken(word='MLB', ner='ORG', idx=(296, 299))\n",
      "NerToken(word='曼佛瑞德', ner='PERSON', idx=(301, 305))\n",
      "NerToken(word='大聯盟', ner='ORG', idx=(320, 323))\n",
      "NerToken(word='大谷翔平', ner='PERSON', idx=(356, 360))\n",
      "NerToken(word='洋基傳奇隊長', ner='ORG', idx=(374, 380))\n",
      "NerToken(word='DerekJeter', ner='PERSON', idx=(380, 390))\n",
      "NerToken(word='大谷翔平', ner='PERSON', idx=(394, 398))\n",
      "NerToken(word='大谷翔平', ner='PERSON', idx=(459, 463))\n",
      "NerToken(word='大谷', ner='PERSON', idx=(480, 482))\n",
      "NerToken(word='天使隊大聯盟', ner='ORG', idx=(504, 510))\n",
      "NerToken(word='大谷', ner='GPE', idx=(519, 521))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618913\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.50s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國', ner='GPE', idx=(11, 13))\n",
      "NerToken(word='新加坡', ner='GPE', idx=(14, 17))\n",
      "NerToken(word='台灣', ner='GPE', idx=(47, 49))\n",
      "NerToken(word='陳時中', ner='PERSON', idx=(60, 63))\n",
      "NerToken(word='中研院', ner='ORG', idx=(81, 84))\n",
      "NerToken(word='陳培哲', ner='PERSON', idx=(86, 89))\n",
      "NerToken(word='新加坡', ner='GPE', idx=(129, 132))\n",
      "NerToken(word='鄧偉堅', ner='PERSON', idx=(136, 139))\n",
      "NerToken(word='鄧', ner='PERSON', idx=(173, 174))\n",
      "NerToken(word='新加坡', ner='GPE', idx=(204, 207))\n",
      "NerToken(word='新加坡', ner='GPE', idx=(227, 230))\n",
      "NerToken(word='新加坡', ner='GPE', idx=(242, 245))\n",
      "NerToken(word='台灣', ner='GPE', idx=(259, 261))\n",
      "NerToken(word='美國', ner='GPE', idx=(305, 307))\n",
      "NerToken(word='以色列', ner='GPE', idx=(308, 311))\n",
      "NerToken(word='法國', ner='GPE', idx=(312, 314))\n",
      "NerToken(word='德國', ner='GPE', idx=(315, 317))\n",
      "NerToken(word='台灣', ner='GPE', idx=(319, 321))\n",
      "NerToken(word='中研院', ner='ORG', idx=(327, 330))\n",
      "NerToken(word='陳培哲', ner='PERSON', idx=(332, 335))\n",
      "NerToken(word='中研院', ner='ORG', idx=(339, 342))\n",
      "NerToken(word='陳培哲', ner='PERSON', idx=(344, 347))\n",
      "NerToken(word='陳培哲', ner='PERSON', idx=(486, 489))\n",
      "NerToken(word='台灣', ner='GPE', idx=(498, 500))\n",
      "NerToken(word='中研院', ner='ORG', idx=(539, 542))\n",
      "NerToken(word='陳培哲', ner='PERSON', idx=(544, 547))\n",
      "NerToken(word='陳時中', ner='PERSON', idx=(641, 644))\n",
      "NerToken(word='台灣', ner='GPE', idx=(680, 682))\n",
      "NerToken(word='TVBS', ner='WORK_OF_ART', idx=(693, 697))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(704, 708))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(711, 714))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(747, 759))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1618919\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.13it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.83it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.27it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.22it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  5.48it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.02it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 11.52it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 10.55it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 500.99it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='勇士隊', ner='ORG', idx=(11, 14))\n",
      "NerToken(word='索勒', ner='PERSON', idx=(29, 31))\n",
      "NerToken(word='杜瓦', ner='PERSON', idx=(50, 52))\n",
      "NerToken(word='勇士隊', ner='ORG', idx=(62, 65))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1619061\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.00s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='深圳特區報', ner='ORG', idx=(149, 154))\n",
      "NerToken(word='大陸', ner='GPE', idx=(161, 163))\n",
      "NerToken(word='肖', ner='PERSON', idx=(165, 166))\n",
      "NerToken(word='肖男', ner='PERSON', idx=(250, 252))\n",
      "NerToken(word='肖男', ner='PERSON', idx=(323, 325))\n",
      "NerToken(word='肖男', ner='PERSON', idx=(411, 413))\n",
      "NerToken(word='肖男', ner='PERSON', idx=(537, 539))\n",
      "NerToken(word='肖男', ner='PERSON', idx=(681, 683))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1619089\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1068.61it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國職棒世界大賽', ner='EVENT', idx=(0, 8))\n",
      "NerToken(word='勇士隊', ner='ORG', idx=(12, 15))\n",
      "NerToken(word='太空人隊', ner='ORG', idx=(49, 53))\n",
      "NerToken(word='美國職棒世界大賽', ner='EVENT', idx=(70, 78))\n",
      "NerToken(word='勇士隊', ner='ORG', idx=(96, 99))\n",
      "NerToken(word='Soler', ner='PERSON', idx=(103, 108))\n",
      "NerToken(word='陽春', ner='GPE', idx=(171, 173))\n",
      "NerToken(word='Soler', ner='PERSON', idx=(177, 182))\n",
      "NerToken(word='世界大賽', ner='EVENT', idx=(190, 194))\n",
      "NerToken(word='Riley', ner='PERSON', idx=(232, 237))\n",
      "NerToken(word='McCormick', ner='PERSON', idx=(253, 262))\n",
      "NerToken(word='Valdez', ner='PERSON', idx=(369, 375))\n",
      "NerToken(word='亞特蘭大', ner='PERSON', idx=(403, 407))\n",
      "NerToken(word='Duvall', ner='PRODUCT', idx=(417, 423))\n",
      "NerToken(word='Morton', ner='PERSON', idx=(471, 477))\n",
      "NerToken(word='勇士隊', ner='ORG', idx=(577, 580))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1619142\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.62s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.07s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 32.34it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.34s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.59s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.53s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.42s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='台灣', ner='GPE', idx=(29, 31))\n",
      "NerToken(word='台北慈濟醫院', ner='ORG', idx=(73, 79))\n",
      "NerToken(word='林亞萱', ner='PERSON', idx=(82, 85))\n",
      "NerToken(word='林亞萱', ner='PERSON', idx=(136, 139))\n",
      "NerToken(word='陳', ner='PERSON', idx=(174, 175))\n",
      "NerToken(word='陳', ner='PERSON', idx=(233, 234))\n",
      "NerToken(word='陳', ner='PERSON', idx=(276, 277))\n",
      "NerToken(word='麥門冬', ner='PERSON', idx=(355, 358))\n",
      "NerToken(word='林亞萱', ner='PERSON', idx=(548, 551))\n",
      "NerToken(word='中醫', ner='NORP', idx=(627, 629))\n",
      "NerToken(word='林亞萱', ner='PERSON', idx=(674, 677))\n",
      "NerToken(word='梨子', ner='PRODUCT', idx=(725, 727))\n",
      "NerToken(word='林亞萱', ner='PERSON', idx=(818, 821))\n",
      "NerToken(word='中醫', ner='NORP', idx=(880, 882))\n",
      "NerToken(word='林亞萱', ner='PERSON', idx=(986, 989))\n",
      "NerToken(word='中醫', ner='ORG', idx=(990, 992))\n",
      "NerToken(word='中醫會', ner='ORG', idx=(1092, 1095))\n",
      "NerToken(word='林亞萱', ner='PERSON', idx=(1172, 1175))\n",
      "NerToken(word='台北慈濟醫院', ner='ORG', idx=(1261, 1267))\n",
      "NerToken(word='中醫', ner='NORP', idx=(1297, 1299))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(1315, 1320))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1619186\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.28s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.19s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.07s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='紐約尼克隊', ner='ORG', idx=(0, 5))\n",
      "NerToken(word='費城', ner='GPE', idx=(29, 31))\n",
      "NerToken(word='尼克', ner='PERSON', idx=(58, 60))\n",
      "NerToken(word='NBA', ner='ORG', idx=(61, 64))\n",
      "NerToken(word='費城', ner='GPE', idx=(98, 100))\n",
      "NerToken(word='尼克隊', ner='ORG', idx=(107, 110))\n",
      "NerToken(word='Kemba Walker', ner='PERSON', idx=(133, 145))\n",
      "NerToken(word='Thank you', ner='PERSON', idx=(169, 178))\n",
      "NerToken(word='尼克', ner='PERSON', idx=(239, 241))\n",
      "NerToken(word='Gibson', ner='PERSON', idx=(249, 255))\n",
      "NerToken(word='Gibson', ner='PERSON', idx=(264, 270))\n",
      "NerToken(word='紐約', ner='GPE', idx=(293, 295))\n",
      "NerToken(word='尼克', ner='PERSON', idx=(305, 307))\n",
      "NerToken(word='Korkmaz', ner='PERSON', idx=(400, 407))\n",
      "NerToken(word='尼克', ner='PERSON', idx=(440, 442))\n",
      "NerToken(word='Rose', ner='ORG', idx=(455, 459))\n",
      "NerToken(word='Randle', ner='PERSON', idx=(472, 478))\n",
      "NerToken(word='尼克隊', ner='ORG', idx=(498, 501))\n",
      "NerToken(word='76人隊', ner='ORG', idx=(572, 576))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1619196\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='祖克柏', ner='PERSON', idx=(100, 103))\n",
      "NerToken(word='商業內幕', ner='ORG', idx=(134, 138))\n",
      "NerToken(word='祖克柏', ner='PERSON', idx=(160, 163))\n",
      "NerToken(word='Instagram', ner='ORG', idx=(244, 253))\n",
      "NerToken(word='祖克柏', ner='PERSON', idx=(302, 305))\n",
      "NerToken(word='Reels', ner='PRODUCT', idx=(343, 348))\n",
      "NerToken(word='TikTok', ner='PRODUCT', idx=(357, 363))\n",
      "NerToken(word='TikTok', ner='ORG', idx=(378, 384))\n",
      "NerToken(word='祖克柏', ner='PERSON', idx=(466, 469))\n",
      "NerToken(word='IG者', ner='PRODUCT', idx=(543, 546))\n",
      "NerToken(word='Snapchat', ner='PERSON', idx=(594, 602))\n",
      "NerToken(word='TikTok', ner='ORG', idx=(607, 613))\n",
      "NerToken(word='二戰', ner='EVENT', idx=(637, 639))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1619233\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.19s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='\\n原PO', ner='PERSON', idx=(131, 135))\n",
      "NerToken(word='家常菜', ner='WORK_OF_ART', idx=(141, 144))\n",
      "NerToken(word='韓國', ner='GPE', idx=(326, 328))\n",
      "NerToken(word='黃浩瑞', ner='PERSON', idx=(481, 484))\n",
      "NerToken(word='中醫', ner='NORP', idx=(514, 516))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1619246\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.80s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 867.49it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='杜業豐', ner='PERSON', idx=(123, 126))\n",
      "NerToken(word='杜業豐', ner='PERSON', idx=(304, 307))\n",
      "NerToken(word='杜業豐', ner='PERSON', idx=(590, 593))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(645, 650))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1619302\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 64.00it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='小玉', ner='PERSON', idx=(4, 6))\n",
      "NerToken(word='AuthMe', ner='ORG', idx=(129, 135))\n",
      "NerToken(word='金管會', ner='ORG', idx=(289, 292))\n",
      "NerToken(word='黃天牧', ner='PERSON', idx=(292, 295))\n",
      "NerToken(word='AuthMe CEOAuthMe', ner='PERSON', idx=(300, 316))\n",
      "NerToken(word='金管會', ner='ORG', idx=(317, 320))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1619426\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='\\n3', ner='PRODUCT', idx=(131, 133))\n",
      "NerToken(word='陽明交大', ner='ORG', idx=(175, 179))\n",
      "NerToken(word='蔡英傑', ner='PERSON', idx=(188, 191))\n",
      "NerToken(word='蔡英傑', ner='PERSON', idx=(262, 265))\n",
      "NerToken(word='馬偕醫院', ner='ORG', idx=(272, 276))\n",
      "NerToken(word='蔡英傑', ner='PERSON', idx=(369, 372))\n",
      "NerToken(word='HOT', ner='ORG', idx=(552, 555))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1619517\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 334.34it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.53it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.44s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.28s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='黃軒', ner='PERSON', idx=(81, 83))\n",
      "NerToken(word='黃軒', ner='PERSON', idx=(122, 124))\n",
      "NerToken(word='黃軒醫師 Dr. Ooi Hean', ner='WORK_OF_ART', idx=(130, 147))\n",
      "NerToken(word='黃軒', ner='PERSON', idx=(298, 300))\n",
      "NerToken(word='黃軒', ner='PERSON', idx=(569, 571))\n",
      "NerToken(word='黃軒', ner='PERSON', idx=(725, 727))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1620093\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='夏洛特黃蜂隊', ner='ORG', idx=(0, 6))\n",
      "NerToken(word='NBA', ner='ORG', idx=(7, 10))\n",
      "NerToken(word='布里吉斯', ner='PERSON', idx=(24, 28))\n",
      "NerToken(word='海沃德', ner='PERSON', idx=(31, 34))\n",
      "NerToken(word='魔術隊', ner='ORG', idx=(75, 78))\n",
      "NerToken(word='Hayward', ner='PERSON', idx=(142, 149))\n",
      "NerToken(word='Anthony', ner='PERSON', idx=(150, 157))\n",
      "NerToken(word='Hayward', ner='PERSON', idx=(165, 172))\n",
      "NerToken(word='魔術隊', ner='ORG', idx=(191, 194))\n",
      "NerToken(word='Gary Harris', ner='PERSON', idx=(247, 258))\n",
      "NerToken(word='黃蜂隊', ner='ORG', idx=(298, 301))\n",
      "NerToken(word='McDaniels', ner='PERSON', idx=(310, 319))\n",
      "NerToken(word='Ball', ner='PRODUCT', idx=(366, 370))\n",
      "NerToken(word='黃蜂隊', ner='ORG', idx=(481, 484))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1620188\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 500.99it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國職棒世界大賽', ner='EVENT', idx=(0, 8))\n",
      "NerToken(word='太空人隊', ner='ORG', idx=(12, 16))\n",
      "NerToken(word='太空人隊', ner='ORG', idx=(61, 65))\n",
      "NerToken(word='Jose Urquidy', ner='PERSON', idx=(69, 81))\n",
      "NerToken(word='High Fastball', ner='PRODUCT', idx=(98, 111))\n",
      "NerToken(word=\"Travisd' Arnaud\", ner='ORG', idx=(181, 196))\n",
      "NerToken(word='勇士隊', ner='ORG', idx=(203, 206))\n",
      "NerToken(word='Siri', ner='PERSON', idx=(245, 249))\n",
      "NerToken(word='太空人隊', ner='ORG', idx=(271, 275))\n",
      "NerToken(word='Gurriel', ner='PERSON', idx=(317, 324))\n",
      "NerToken(word='Siri', ner='PERSON', idx=(349, 353))\n",
      "NerToken(word='Brantley', ner='PERSON', idx=(447, 455))\n",
      "NerToken(word='Urquidy', ner='PERSON', idx=(488, 495))\n",
      "NerToken(word='勇士隊', ner='ORG', idx=(505, 508))\n",
      "NerToken(word='Altuve', ner='PERSON', idx=(543, 549))\n",
      "NerToken(word='Altuve', ner='PERSON', idx=(557, 563))\n",
      "NerToken(word='Bernie Williams', ner='PERSON', idx=(583, 598))\n",
      "NerToken(word='勇士隊', ner='ORG', idx=(619, 622))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1620296\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.95s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.54s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='李其昀', ner='PERSON', idx=(34, 37))\n",
      "NerToken(word='李其昀', ner='PERSON', idx=(58, 61))\n",
      "NerToken(word='李其昀', ner='PERSON', idx=(168, 171))\n",
      "NerToken(word='李其昀', ner='PERSON', idx=(303, 306))\n",
      "NerToken(word='李其昀', ner='PERSON', idx=(500, 503))\n",
      "NerToken(word='高GI', ner='PRODUCT', idx=(519, 522))\n",
      "NerToken(word='李其昀', ner='PERSON', idx=(631, 634))\n",
      "NerToken(word='李其昀', ner='PERSON', idx=(746, 749))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(798, 803))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1620380\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.38s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.39s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.30s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.38s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.23s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='蔣志偉', ner='PERSON', idx=(149, 152))\n",
      "NerToken(word='長庚醫院', ner='ORG', idx=(237, 241))\n",
      "NerToken(word='謝佳訓', ner='PERSON', idx=(248, 251))\n",
      "NerToken(word='林口長庚醫院', ner='ORG', idx=(462, 468))\n",
      "NerToken(word='吳益銘', ner='PERSON', idx=(471, 474))\n",
      "NerToken(word='高雄市立大同醫院家醫科', ner='ORG', idx=(565, 576))\n",
      "NerToken(word='沈政廷', ner='PERSON', idx=(580, 583))\n",
      "NerToken(word='林口長庚醫院', ner='ORG', idx=(712, 718))\n",
      "NerToken(word='吳益銘', ner='PERSON', idx=(721, 724))\n",
      "NerToken(word='蔣志偉', ner='PERSON', idx=(829, 832))\n",
      "NerToken(word='高雄市立大同醫院健康管理中心', ner='ORG', idx=(896, 910))\n",
      "NerToken(word='鄭涵芸', ner='PERSON', idx=(912, 915))\n",
      "NerToken(word='成大醫院腫瘤學部', ner='ORG', idx=(1040, 1048))\n",
      "NerToken(word='蘇文彬', ner='PERSON', idx=(1052, 1055))\n",
      "NerToken(word='臺灣', ner='NORP', idx=(1057, 1059))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1620620\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='波士頓', ner='GPE', idx=(7, 10))\n",
      "NerToken(word='坎特', ner='PERSON', idx=(16, 18))\n",
      "NerToken(word='大陸', ner='GPE', idx=(27, 29))\n",
      "NerToken(word='藏人', ner='NORP', idx=(33, 35))\n",
      "NerToken(word='維吾爾族人', ner='NORP', idx=(36, 41))\n",
      "NerToken(word='香港', ner='GPE', idx=(58, 60))\n",
      "NerToken(word='台灣', ner='GPE', idx=(64, 66))\n",
      "NerToken(word='坎特（Enes Kanter）', ner='PERSON', idx=(75, 90))\n",
      "NerToken(word='西藏', ner='GPE', idx=(131, 133))\n",
      "NerToken(word='香港', ner='GPE', idx=(146, 148))\n",
      "NerToken(word='台灣', ner='GPE', idx=(152, 154))\n",
      "NerToken(word='坎特', ner='PERSON', idx=(160, 162))\n",
      "NerToken(word='中國', ner='GPE', idx=(194, 196))\n",
      "NerToken(word='西藏', ner='GPE', idx=(199, 201))\n",
      "NerToken(word='西藏人民', ner='NORP', idx=(203, 207))\n",
      "NerToken(word='騰訊', ner='ORG', idx=(213, 215))\n",
      "NerToken(word='塞爾蒂克', ner='EVENT', idx=(223, 227))\n",
      "NerToken(word='大陸外交部', ner='ORG', idx=(230, 235))\n",
      "NerToken(word='坎特', ner='PERSON', idx=(255, 257))\n",
      "NerToken(word='大陸', ner='GPE', idx=(265, 267))\n",
      "NerToken(word='習近平', ner='PERSON', idx=(270, 273))\n",
      "NerToken(word='中國共產黨', ner='ORG', idx=(274, 279))\n",
      "NerToken(word='維吾爾族人', ner='NORP', idx=(301, 306))\n",
      "NerToken(word='坎特', ner='PERSON', idx=(318, 320))\n",
      "NerToken(word='耐吉（Nike）', ner='ORG', idx=(330, 338))\n",
      "NerToken(word='奈特', ner='PERSON', idx=(341, 343))\n",
      "NerToken(word='大陸', ner='GPE', idx=(367, 369))\n",
      "NerToken(word='詹姆斯', ner='PERSON', idx=(391, 394))\n",
      "NerToken(word='喬丹', ner='PERSON', idx=(415, 417))\n",
      "NerToken(word='坎特', ner='PERSON', idx=(436, 438))\n",
      "NerToken(word='瑞士', ner='GPE', idx=(440, 442))\n",
      "NerToken(word='土耳其', ner='GPE', idx=(444, 447))\n",
      "NerToken(word='穆斯林', ner='NORP', idx=(454, 457))\n",
      "NerToken(word='NBA', ner='ORG', idx=(463, 466))\n",
      "NerToken(word='猶他爵士隊', ner='ORG', idx=(476, 481))\n",
      "NerToken(word='土耳其', ner='GPE', idx=(487, 490))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NerToken(word='艾爾段', ner='PERSON', idx=(492, 495))\n",
      "NerToken(word='土耳其', ner='GPE', idx=(535, 538))\n",
      "NerToken(word='土耳其法院', ner='ORG', idx=(547, 552))\n",
      "NerToken(word='INTERPOL', ner='ORG', idx=(580, 588))\n",
      "NerToken(word='坎特', ner='PERSON', idx=(613, 615))\n",
      "NerToken(word='中央社', ner='ORG', idx=(622, 625))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1620621\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='衛生福利部', ner='ORG', idx=(55, 60))\n",
      "NerToken(word='衛福部', ner='ORG', idx=(109, 112))\n",
      "NerToken(word='食藥署', ner='ORG', idx=(224, 227))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1620669\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.72s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='祖克柏', ner='PERSON', idx=(22, 25))\n",
      "NerToken(word='Meta', ner='ORG', idx=(56, 60))\n",
      "NerToken(word='歐盟', ner='ORG', idx=(83, 85))\n",
      "NerToken(word='Meta\\u3000', ner='ORG', idx=(176, 181))\n",
      "NerToken(word='美國', ner='GPE', idx=(206, 208))\n",
      "NerToken(word='潰雪', ner='WORK_OF_ART', idx=(213, 215))\n",
      "NerToken(word='微軟', ner='ORG', idx=(462, 464))\n",
      "NerToken(word='HOT', ner='ORG', idx=(704, 707))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1620685\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.85s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.88s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.87s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='英國', ner='NORP', idx=(2, 4))\n",
      "NerToken(word='英國廣播公司新聞網', ner='ORG', idx=(80, 89))\n",
      "NerToken(word='刺胳針傳染病期刊', ner='WORK_OF_ART', idx=(206, 214))\n",
      "NerToken(word='英國', ner='GPE', idx=(357, 359))\n",
      "NerToken(word='COVID-19', ner='PRODUCT', idx=(422, 430))\n",
      "NerToken(word='倫敦', ner='GPE', idx=(492, 494))\n",
      "NerToken(word='波爾頓', ner='GPE', idx=(495, 498))\n",
      "NerToken(word='倫敦帝國學院', ner='ORG', idx=(546, 552))\n",
      "NerToken(word='Imperial College London', ner='ORG', idx=(553, 576))\n",
      "NerToken(word='拉瓦尼', ner='PERSON', idx=(579, 582))\n",
      "NerToken(word='中央社', ner='ORG', idx=(683, 686))\n",
      "NerToken(word='《TVBS》', ner='WORK_OF_ART', idx=(687, 693))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(699, 703))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(706, 709))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(742, 754))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1620748\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='吳映澄', ner='PERSON', idx=(101, 104))\n",
      "NerToken(word='台灣', ner='GPE', idx=(109, 111))\n",
      "NerToken(word='食品添加物使用範圍及限量暨規格標準', ner='LAW', idx=(114, 131))\n",
      "NerToken(word='吳映澄', ner='PERSON', idx=(203, 206))\n",
      "NerToken(word='吳映澄', ner='PERSON', idx=(246, 249))\n",
      "NerToken(word='吳映澄', ner='PERSON', idx=(333, 336))\n",
      "NerToken(word='吳映澄', ner='PERSON', idx=(364, 367))\n",
      "NerToken(word='吳映澄', ner='PERSON', idx=(437, 440))\n",
      "NerToken(word='台灣', ner='GPE', idx=(464, 466))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(494, 499))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1620761\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='劉怡里', ner='PERSON', idx=(91, 94))\n",
      "NerToken(word='劉怡里', ner='PERSON', idx=(226, 229))\n",
      "NerToken(word='劉怡里', ner='PERSON', idx=(355, 358))\n",
      "NerToken(word='劉怡里', ner='PERSON', idx=(537, 540))\n",
      "NerToken(word='劉怡里', ner='PERSON', idx=(678, 681))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(732, 737))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1620858\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.16s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.17s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='彰化', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='彰化醫院', ner='ORG', idx=(160, 164))\n",
      "NerToken(word='呂明川', ner='PERSON', idx=(170, 173))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1620877\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.30s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='台灣', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='周天成', ner='PERSON', idx=(6, 9))\n",
      "NerToken(word='法國羽球公開賽', ner='EVENT', idx=(14, 21))\n",
      "NerToken(word='法國', ner='GPE', idx=(59, 61))\n",
      "NerToken(word='利弗德斯', ner='PERSON', idx=(63, 67))\n",
      "NerToken(word='法國羽球公開賽', ner='EVENT', idx=(94, 101))\n",
      "NerToken(word='世界羽球巡迴賽', ner='EVENT', idx=(103, 110))\n",
      "NerToken(word='超級1000', ner='EVENT', idx=(123, 129))\n",
      "NerToken(word='法國巴黎', ner='GPE', idx=(139, 143))\n",
      "NerToken(word='周天成', ner='PERSON', idx=(154, 157))\n",
      "NerToken(word='法國', ner='GPE', idx=(179, 181))\n",
      "NerToken(word='利弗德斯', ner='PERSON', idx=(183, 187))\n",
      "NerToken(word='周天成', ner='PERSON', idx=(223, 226))\n",
      "NerToken(word='周天成', ner='PERSON', idx=(240, 243))\n",
      "NerToken(word='周天成', ner='PERSON', idx=(277, 280))\n",
      "NerToken(word='周天成', ner='PERSON', idx=(307, 310))\n",
      "NerToken(word='周天成', ner='PERSON', idx=(369, 372))\n",
      "NerToken(word='周天成', ner='PERSON', idx=(403, 406))\n",
      "NerToken(word='王子維', ner='PERSON', idx=(447, 450))\n",
      "NerToken(word='韓國', ner='GPE', idx=(475, 477))\n",
      "NerToken(word='許侊熙', ner='PERSON', idx=(479, 482))\n",
      "NerToken(word='中央社', ner='ORG', idx=(489, 492))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1621002\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.88s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.90s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.17s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='蕭瑋霖', ner='PERSON', idx=(77, 80))\n",
      "NerToken(word='蕭瑋霖', ner='PERSON', idx=(141, 144))\n",
      "NerToken(word='蕭瑋霖', ner='PERSON', idx=(336, 339))\n",
      "NerToken(word='蕭瑋霖', ner='PERSON', idx=(450, 453))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1621281\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.88s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='林依晨', ner='PERSON', idx=(8, 11))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(118, 123))\n",
      "NerToken(word='韋汝', ner='PERSON', idx=(127, 129))\n",
      "NerToken(word='萬芳醫院', ner='ORG', idx=(160, 164))\n",
      "NerToken(word='王樂明', ner='PERSON', idx=(169, 172))\n",
      "NerToken(word='林志玲', ner='PERSON', idx=(294, 297))\n",
      "NerToken(word='張本渝', ner='PERSON', idx=(298, 301))\n",
      "NerToken(word='羅巧倫', ner='PERSON', idx=(302, 305))\n",
      "NerToken(word='王樂明', ner='PERSON', idx=(329, 332))\n",
      "NerToken(word='王樂明', ner='PERSON', idx=(439, 442))\n",
      "NerToken(word='北醫', ner='ORG', idx=(674, 676))\n",
      "NerToken(word='許藍方', ner='PERSON', idx=(681, 684))\n",
      "NerToken(word='許藍方', ner='PERSON', idx=(764, 767))\n",
      "NerToken(word='《健康2.0', ner='WORK_OF_ART', idx=(935, 941))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1621283\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.18s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.10s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.74s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.83s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.73s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='中山醫院', ner='ORG', idx=(119, 123))\n",
      "NerToken(word='陳欣湄', ner='PERSON', idx=(128, 131))\n",
      "NerToken(word='陳欣湄', ner='PERSON', idx=(229, 232))\n",
      "NerToken(word='陳欣湄', ner='PERSON', idx=(354, 357))\n",
      "NerToken(word='陳欣湄', ner='PERSON', idx=(449, 452))\n",
      "NerToken(word='陳欣湄', ner='PERSON', idx=(507, 510))\n",
      "NerToken(word='陳欣湄', ner='PERSON', idx=(803, 806))\n",
      "NerToken(word='陳欣湄', ner='PERSON', idx=(945, 948))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(1005, 1010))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1621366\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.74s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 500.99it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.01s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.15s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.34s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.13s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='台灣', ner='NORP', idx=(0, 2))\n",
      "NerToken(word='台灣', ner='GPE', idx=(15, 17))\n",
      "NerToken(word='農委會', ner='ORG', idx=(49, 52))\n",
      "NerToken(word='臺中農改場', ner='ORG', idx=(156, 161))\n",
      "NerToken(word='國泰醫院', ner='ORG', idx=(164, 168))\n",
      "NerToken(word='陽明大學', ner='ORG', idx=(256, 260))\n",
      "NerToken(word='臺中農改場', ner='ORG', idx=(358, 363))\n",
      "NerToken(word='大麥', ner='PERSON', idx=(488, 490))\n",
      "NerToken(word='薏仁', ner='PERSON', idx=(524, 526))\n",
      "NerToken(word='臺中農改場', ner='ORG', idx=(823, 828))\n",
      "NerToken(word='台中', ner='GPE', idx=(840, 842))\n",
      "NerToken(word='臺中大雅農會', ner='ORG', idx=(875, 881))\n",
      "NerToken(word='彰化二林農會', ner='ORG', idx=(882, 888))\n",
      "NerToken(word='中都農業生產合作社', ner='ORG', idx=(889, 898))\n",
      "NerToken(word='喜願行', ner='ORG', idx=(899, 902))\n",
      "NerToken(word='臺中農改場', ner='FAC', idx=(960, 965))\n",
      "NerToken(word='微波', ner='GPE', idx=(1020, 1022))\n",
      "NerToken(word='台灣', ner='GPE', idx=(1045, 1047))\n",
      "NerToken(word='臺中區農業改良場', ner='ORG', idx=(1123, 1131))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(1185, 1190))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623511\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.86s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.74s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.95s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.90s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.87s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='泰國', ner='GPE', idx=(9, 11))\n",
      "NerToken(word='曼谷蘇凡納布機場', ner='FAC', idx=(44, 52))\n",
      "NerToken(word='泰', ner='GPE', idx=(58, 59))\n",
      "NerToken(word='泰國政府', ner='ORG', idx=(62, 66))\n",
      "NerToken(word='泰國', ner='GPE', idx=(107, 109))\n",
      "NerToken(word='泰國', ner='GPE', idx=(137, 139))\n",
      "NerToken(word='普吉島', ner='LOC', idx=(140, 143))\n",
      "NerToken(word='普吉島', ner='GPE', idx=(160, 163))\n",
      "NerToken(word='蘇美島', ner='LOC', idx=(190, 193))\n",
      "NerToken(word='泰國', ner='NORP', idx=(203, 205))\n",
      "NerToken(word='泰國', ner='GPE', idx=(264, 266))\n",
      "NerToken(word='中國', ner='GPE', idx=(279, 281))\n",
      "NerToken(word='香港', ner='GPE', idx=(282, 284))\n",
      "NerToken(word='美國', ner='GPE', idx=(285, 287))\n",
      "NerToken(word='英國', ner='GPE', idx=(288, 290))\n",
      "NerToken(word='澳洲', ner='GPE', idx=(291, 293))\n",
      "NerToken(word='紐西蘭', ner='GPE', idx=(294, 297))\n",
      "NerToken(word='新加坡', ner='GPE', idx=(298, 301))\n",
      "NerToken(word='韓國', ner='GPE', idx=(302, 304))\n",
      "NerToken(word='日本', ner='GPE', idx=(305, 307))\n",
      "NerToken(word='台灣', ner='GPE', idx=(308, 310))\n",
      "NerToken(word='歐盟', ner='ORG', idx=(311, 313))\n",
      "NerToken(word='曼谷', ner='GPE', idx=(319, 321))\n",
      "NerToken(word='泰國', ner='GPE', idx=(324, 326))\n",
      "NerToken(word='蘇凡納布機場', ner='FAC', idx=(331, 337))\n",
      "NerToken(word='蘇凡納布機場', ner='FAC', idx=(383, 389))\n",
      "NerToken(word='泰國', ner='GPE', idx=(492, 494))\n",
      "NerToken(word='英國', ner='GPE', idx=(518, 520))\n",
      "NerToken(word='哈珊（Awat hassan）', ner='PERSON', idx=(521, 536))\n",
      "NerToken(word='泰國', ner='GPE', idx=(541, 543))\n",
      "NerToken(word='泰國', ner='GPE', idx=(548, 550))\n",
      "NerToken(word='泰籍', ner='NORP', idx=(554, 556))\n",
      "NerToken(word='泰國', ner='GPE', idx=(562, 564))\n",
      "NerToken(word='哈珊', ner='PERSON', idx=(565, 567))\n",
      "NerToken(word='泰國', ner='GPE', idx=(593, 595))\n",
      "NerToken(word='哈珊', ner='PERSON', idx=(596, 598))\n",
      "NerToken(word='泰籍', ner='NORP', idx=(599, 601))\n",
      "NerToken(word='妮查葳（Nichawee Aeng-Long）', ner='PERSON', idx=(603, 626))\n",
      "NerToken(word='泰國', ner='GPE', idx=(634, 636))\n",
      "NerToken(word='英國', ner='GPE', idx=(637, 639))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NerToken(word='泰國', ner='GPE', idx=(701, 703))\n",
      "NerToken(word='泰國', ner='GPE', idx=(712, 714))\n",
      "NerToken(word='泰國民航局', ner='ORG', idx=(726, 731))\n",
      "NerToken(word='蘇凡納布機場', ner='FAC', idx=(769, 775))\n",
      "NerToken(word='泰國觀光局', ner='ORG', idx=(776, 781))\n",
      "NerToken(word='泰國', ner='GPE', idx=(819, 821))\n",
      "NerToken(word='泰國', ner='GPE', idx=(872, 874))\n",
      "NerToken(word='中央社', ner='ORG', idx=(899, 902))\n",
      "NerToken(word='《TVBS》', ner='WORK_OF_ART', idx=(903, 909))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(915, 919))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(922, 925))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(958, 970))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623564\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.73s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 334.21it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='以色列', ner='GPE', idx=(11, 14))\n",
      "NerToken(word='猶太人', ner='NORP', idx=(39, 42))\n",
      "NerToken(word='巴勒斯坦人', ner='NORP', idx=(43, 48))\n",
      "NerToken(word='法新社', ner='ORG', idx=(69, 72))\n",
      "NerToken(word='以色列', ner='GPE', idx=(76, 79))\n",
      "NerToken(word='耶路撒冷', ner='GPE', idx=(82, 86))\n",
      "NerToken(word='巴勒斯坦裔', ner='NORP', idx=(91, 96))\n",
      "NerToken(word='拉祖克', ner='PERSON', idx=(103, 106))\n",
      "NerToken(word='拉祖克', ner='GPE', idx=(166, 169))\n",
      "NerToken(word='以色列', ner='GPE', idx=(170, 173))\n",
      "NerToken(word='耶路撒冷', ner='GPE', idx=(207, 211))\n",
      "NerToken(word='哈山-納荷姆', ner='PERSON', idx=(214, 220))\n",
      "NerToken(word='以色列', ner='GPE', idx=(292, 295))\n",
      "NerToken(word='以色列', ner='GPE', idx=(322, 325))\n",
      "NerToken(word='以色列', ner='GPE', idx=(361, 364))\n",
      "NerToken(word='以色列', ner='GPE', idx=(413, 416))\n",
      "NerToken(word='以色列', ner='GPE', idx=(450, 453))\n",
      "NerToken(word='以色列觀光部', ner='ORG', idx=(470, 476))\n",
      "NerToken(word='哈勒維（Amir Halevi）', ner='PERSON', idx=(479, 495))\n",
      "NerToken(word='法新社', ner='ORG', idx=(499, 502))\n",
      "NerToken(word='中央社', ner='ORG', idx=(581, 584))\n",
      "NerToken(word='《TVBS》', ner='WORK_OF_ART', idx=(585, 591))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(597, 601))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(604, 607))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(640, 652))\n",
      "NerToken(word='HOT', ner='ORG', idx=(654, 657))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623565\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.83s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 63.99it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.41s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='日本', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='眾議院', ner='ORG', idx=(6, 9))\n",
      "NerToken(word='自由民主黨', ner='ORG', idx=(17, 22))\n",
      "NerToken(word='甘利明', ner='PERSON', idx=(25, 28))\n",
      "NerToken(word='自民黨', ner='ORG', idx=(45, 48))\n",
      "NerToken(word='岸田文雄', ner='PERSON', idx=(50, 54))\n",
      "NerToken(word='茂木敏充', ner='PERSON', idx=(61, 65))\n",
      "NerToken(word='林芳正', ner='PERSON', idx=(71, 74))\n",
      "NerToken(word='自民黨', ner='ORG', idx=(87, 90))\n",
      "NerToken(word='甘利', ner='PERSON', idx=(93, 95))\n",
      "NerToken(word='神奈川13區', ner='LOC', idx=(99, 105))\n",
      "NerToken(word='眾議院', ner='ORG', idx=(108, 111))\n",
      "NerToken(word='立憲民主黨', ner='ORG', idx=(121, 126))\n",
      "NerToken(word='太榮志', ner='PERSON', idx=(132, 135))\n",
      "NerToken(word='自民黨', ner='ORG', idx=(140, 143))\n",
      "NerToken(word='岸田文雄', ner='PERSON', idx=(150, 154))\n",
      "NerToken(word='茂木敏充', ner='PERSON', idx=(163, 167))\n",
      "NerToken(word='自民黨', ner='ORG', idx=(184, 187))\n",
      "NerToken(word='岸田文雄', ner='PERSON', idx=(221, 225))\n",
      "NerToken(word='日本', ner='GPE', idx=(268, 270))\n",
      "NerToken(word='產經新聞', ner='ORG', idx=(270, 274))\n",
      "NerToken(word='茂木', ner='PERSON', idx=(277, 279))\n",
      "NerToken(word='自民黨', ner='ORG', idx=(281, 284))\n",
      "NerToken(word='教育部長', ner='ORG', idx=(312, 316))\n",
      "NerToken(word='林芳正', ner='PERSON', idx=(317, 320))\n",
      "NerToken(word='眾議院', ner='ORG', idx=(323, 326))\n",
      "NerToken(word='自民黨', ner='ORG', idx=(336, 339))\n",
      "NerToken(word='岸田', ner='PERSON', idx=(354, 356))\n",
      "NerToken(word='茂木', ner='PERSON', idx=(357, 359))\n",
      "NerToken(word='眾議院', ner='ORG', idx=(370, 373))\n",
      "NerToken(word='自民黨', ner='ORG', idx=(376, 379))\n",
      "NerToken(word='岸田', ner='PERSON', idx=(446, 448))\n",
      "NerToken(word='茂木', ner='PERSON', idx=(450, 452))\n",
      "NerToken(word='參議院', ner='ORG', idx=(468, 471))\n",
      "NerToken(word='參議院', ner='ORG', idx=(474, 477))\n",
      "NerToken(word='岸田', ner='PERSON', idx=(486, 488))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NerToken(word='茂木', ner='PERSON', idx=(492, 494))\n",
      "NerToken(word='茂木', ner='PERSON', idx=(539, 541))\n",
      "NerToken(word='眾議院', ner='ORG', idx=(542, 545))\n",
      "NerToken(word='櫪木', ner='GPE', idx=(545, 547))\n",
      "NerToken(word='眾議院', ner='ORG', idx=(569, 572))\n",
      "NerToken(word='日本', ner='GPE', idx=(575, 577))\n",
      "NerToken(word='新黨籍', ner='ORG', idx=(577, 580))\n",
      "NerToken(word='自民黨', ner='ORG', idx=(595, 598))\n",
      "NerToken(word='自民黨', ner='ORG', idx=(621, 624))\n",
      "NerToken(word='安倍晉三', ner='PERSON', idx=(642, 646))\n",
      "NerToken(word='茂木', ner='PERSON', idx=(650, 652))\n",
      "NerToken(word='跨太平洋夥伴協定', ner='LAW', idx=(665, 673))\n",
      "NerToken(word='日', ner='GPE', idx=(682, 683))\n",
      "NerToken(word='美', ner='GPE', idx=(683, 684))\n",
      "NerToken(word='茂木', ner='PERSON', idx=(702, 704))\n",
      "NerToken(word='日本', ner='GPE', idx=(718, 720))\n",
      "NerToken(word='印度太平洋', ner='LOC', idx=(728, 733))\n",
      "NerToken(word='日本', ner='GPE', idx=(809, 811))\n",
      "NerToken(word='台灣', ner='GPE', idx=(818, 820))\n",
      "NerToken(word='自民黨', ner='ORG', idx=(825, 828))\n",
      "NerToken(word='自民黨', ner='ORG', idx=(836, 839))\n",
      "NerToken(word='竹下派', ner='ORG', idx=(840, 843))\n",
      "NerToken(word='茂木', ner='PERSON', idx=(852, 854))\n",
      "NerToken(word='岸田', ner='PERSON', idx=(865, 867))\n",
      "NerToken(word='岸田', ner='PERSON', idx=(890, 892))\n",
      "NerToken(word='茂木', ner='PERSON', idx=(905, 907))\n",
      "NerToken(word='茂木', ner='GPE', idx=(919, 921))\n",
      "NerToken(word='安倍（Abe）晉三', ner='PERSON', idx=(937, 946))\n",
      "NerToken(word='自民黨', ner='ORG', idx=(947, 950))\n",
      "NerToken(word='麻生（Aso）太郎', ner='PERSON', idx=(953, 962))\n",
      "NerToken(word='甘利（Amari）明', ner='PERSON', idx=(963, 973))\n",
      "NerToken(word='中央社', ner='ORG', idx=(979, 982))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623574\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.89s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.96s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.47it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='英國', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='牛津', ner='GPE', idx=(2, 4))\n",
      "NerToken(word='英語', ner='LANGUAGE', idx=(4, 6))\n",
      "NerToken(word='英國廣播公司', ner='ORG', idx=(88, 94))\n",
      "NerToken(word='BBC', ner='ORG', idx=(95, 98))\n",
      "NerToken(word='牛津', ner='GPE', idx=(102, 104))\n",
      "NerToken(word='英語', ner='LANGUAGE', idx=(104, 106))\n",
      "NerToken(word='OED）', ner='ORG', idx=(136, 140))\n",
      "NerToken(word='麥佛遜', ner='PERSON', idx=(144, 147))\n",
      "NerToken(word='vax', ner='PRODUCT', idx=(167, 170))\n",
      "NerToken(word='麥佛遜', ner='PERSON', idx=(198, 201))\n",
      "NerToken(word='vax', ner='PERSON', idx=(323, 326))\n",
      "NerToken(word='牛津', ner='GPE', idx=(364, 366))\n",
      "NerToken(word='英語', ner='LANGUAGE', idx=(366, 368))\n",
      "NerToken(word='vax', ner='PRODUCT', idx=(372, 375))\n",
      "NerToken(word='vax', ner='PERSON', idx=(383, 386))\n",
      "NerToken(word='vax', ner='PERSON', idx=(399, 402))\n",
      "NerToken(word='vaxxie', ner='PERSON', idx=(435, 441))\n",
      "NerToken(word='anti-vax', ner='PERSON', idx=(503, 511))\n",
      "NerToken(word='英文', ner='LANGUAGE', idx=(576, 578))\n",
      "NerToken(word='vaccinate', ner='PRODUCT', idx=(600, 609))\n",
      "NerToken(word='vaccination', ner='PRODUCT', idx=(610, 621))\n",
      "NerToken(word='拉丁字', ner='LANGUAGE', idx=(643, 646))\n",
      "NerToken(word='vacca', ner='PRODUCT', idx=(646, 651))\n",
      "NerToken(word='牛津', ner='GPE', idx=(660, 662))\n",
      "NerToken(word='英語', ner='LANGUAGE', idx=(662, 664))\n",
      "NerToken(word='英國', ner='NORP', idx=(694, 696))\n",
      "NerToken(word='詹納（Edward Jenner）', ner='PERSON', idx=(704, 721))\n",
      "NerToken(word='中央社', ner='ORG', idx=(759, 762))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623579\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.09it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.07it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.24it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='英國', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='伊麗莎白二世', ner='PERSON', idx=(4, 10))\n",
      "NerToken(word='Queen Elizabeth II', ner='PERSON', idx=(11, 29))\n",
      "NerToken(word='英國', ner='GPE', idx=(42, 44))\n",
      "NerToken(word='COP26氣候峰會', ner='EVENT', idx=(47, 56))\n",
      "NerToken(word='英國', ner='GPE', idx=(57, 59))\n",
      "NerToken(word='溫莎城堡', ner='GPE', idx=(73, 77))\n",
      "NerToken(word='英國', ner='NORP', idx=(92, 94))\n",
      "NerToken(word='太陽報', ner='ORG', idx=(97, 100))\n",
      "NerToken(word='綠色捷豹（Jaguar）', ner='PRODUCT', idx=(138, 150))\n",
      "NerToken(word='格拉斯哥', ner='GPE', idx=(213, 217))\n",
      "NerToken(word='中央社', ner='ORG', idx=(229, 232))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623580\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='可口可樂（Coca-Cola）', ner='ORG', idx=(7, 22))\n",
      "NerToken(word='BodyArmor', ner='ORG', idx=(56, 65))\n",
      "NerToken(word='北美', ner='LOC', idx=(76, 78))\n",
      "NerToken(word='法新社', ner='ORG', idx=(90, 93))\n",
      "NerToken(word='可口可樂', ner='ORG', idx=(96, 100))\n",
      "NerToken(word='BodyArmor', ner='ORG', idx=(109, 118))\n",
      "NerToken(word='美國', ner='GPE', idx=(127, 129))\n",
      "NerToken(word='布萊恩', ner='PERSON', idx=(134, 137))\n",
      "NerToken(word='Kobe Bryant', ner='ORG', idx=(138, 149))\n",
      "NerToken(word='BodyArmor', ner='ORG', idx=(159, 168))\n",
      "NerToken(word='可口可樂北美營運部', ner='ORG', idx=(173, 182))\n",
      "NerToken(word='李維拉', ner='PERSON', idx=(184, 187))\n",
      "NerToken(word='BodyArmor', ner='ORG', idx=(207, 216))\n",
      "NerToken(word='可口可樂', ner='PERSON', idx=(259, 263))\n",
      "NerToken(word='BodyArmor', ner='ORG', idx=(267, 276))\n",
      "NerToken(word='BodyArmor', ner='ORG', idx=(315, 324))\n",
      "NerToken(word='瑞波爾', ner='PERSON', idx=(332, 335))\n",
      "NerToken(word='Mike Repole', ner='PERSON', idx=(336, 347))\n",
      "NerToken(word='海斯迪', ner='PERSON', idx=(351, 354))\n",
      "NerToken(word='BodyArmor', ner='ORG', idx=(381, 390))\n",
      "NerToken(word='中央社', ner='ORG', idx=(416, 419))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623601\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(4, 6))\n",
      "NerToken(word='川普', ner='PERSON', idx=(12, 14))\n",
      "NerToken(word='美國', ner='GPE', idx=(18, 20))\n",
      "NerToken(word='巴黎', ner='GPE', idx=(22, 24))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(60, 62))\n",
      "NerToken(word='蘇格蘭格拉斯哥', ner='GPE', idx=(63, 70))\n",
      "NerToken(word='川普', ner='PERSON', idx=(112, 114))\n",
      "NerToken(word='川普', ner='PERSON', idx=(123, 125))\n",
      "NerToken(word='巴黎', ner='GPE', idx=(137, 139))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(143, 145))\n",
      "NerToken(word='美國', ner='GPE', idx=(170, 172))\n",
      "NerToken(word='巴黎氣候協定', ner='LAW', idx=(182, 188))\n",
      "NerToken(word='美國', ner='GPE', idx=(209, 211))\n",
      "NerToken(word='巴黎', ner='GPE', idx=(226, 228))\n",
      "NerToken(word='川普', ner='PERSON', idx=(231, 233))\n",
      "NerToken(word='巴黎', ner='GPE', idx=(240, 242))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(254, 256))\n",
      "NerToken(word='格拉斯哥氣候峰會', ner='EVENT', idx=(259, 267))\n",
      "NerToken(word='美國', ner='GPE', idx=(323, 325))\n",
      "NerToken(word='美國', ner='GPE', idx=(340, 342))\n",
      "NerToken(word='中央社', ner='ORG', idx=(410, 413))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623613\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.60s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.59it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.11s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='台灣', ner='GPE', idx=(6, 8))\n",
      "NerToken(word='莫德納', ner='PRODUCT', idx=(13, 16))\n",
      "NerToken(word='美國', ner='GPE', idx=(29, 31))\n",
      "NerToken(word='國務院', ner='ORG', idx=(31, 34))\n",
      "NerToken(word='台', ner='GPE', idx=(44, 45))\n",
      "NerToken(word='美', ner='GPE', idx=(76, 77))\n",
      "NerToken(word='台', ner='GPE', idx=(77, 78))\n",
      "NerToken(word='美國', ner='NORP', idx=(83, 85))\n",
      "NerToken(word='台灣', ner='GPE', idx=(90, 92))\n",
      "NerToken(word='2019冠狀病毒疾病', ner='EVENT', idx=(101, 111))\n",
      "NerToken(word='台灣', ner='GPE', idx=(121, 123))\n",
      "NerToken(word='莫德納（Moderna）', ner='PRODUCT', idx=(128, 140))\n",
      "NerToken(word='美', ner='GPE', idx=(171, 172))\n",
      "NerToken(word='蕭美琴', ner='PERSON', idx=(174, 177))\n",
      "NerToken(word='肯塔基州路易維爾（Louisville）機場', ner='FAC', idx=(179, 201))\n",
      "NerToken(word='美國國務院', ner='ORG', idx=(208, 213))\n",
      "NerToken(word='桃園國際機場', ner='FAC', idx=(242, 248))\n",
      "NerToken(word='美國在台協會（AIT', ner='ORG', idx=(249, 259))\n",
      "NerToken(word='孫曉雅', ner='PERSON', idx=(262, 265))\n",
      "NerToken(word='中央流行疫情指揮中心', ner='ORG', idx=(282, 292))\n",
      "NerToken(word='陳時中', ner='PERSON', idx=(295, 298))\n",
      "NerToken(word='外交部', ner='ORG', idx=(299, 302))\n",
      "NerToken(word='曾厚仁', ner='PERSON', idx=(306, 309))\n",
      "NerToken(word='國務院', ner='ORG', idx=(319, 322))\n",
      "NerToken(word='COVID-19', ner='PRODUCT', idx=(325, 333))\n",
      "NerToken(word='美', ner='GPE', idx=(361, 362))\n",
      "NerToken(word='台', ner='GPE', idx=(362, 363))\n",
      "NerToken(word='國務院', ner='ORG', idx=(369, 372))\n",
      "NerToken(word='普萊斯（Ned Price）', ner='PERSON', idx=(375, 389))\n",
      "NerToken(word='莫德納', ner='EVENT', idx=(404, 407))\n",
      "NerToken(word='美國', ner='GPE', idx=(422, 424))\n",
      "NerToken(word='台灣', ner='GPE', idx=(425, 427))\n",
      "NerToken(word='台灣', ner='GPE', idx=(433, 435))\n",
      "NerToken(word='美國', ner='GPE', idx=(459, 461))\n",
      "NerToken(word='台', ner='GPE', idx=(464, 465))\n",
      "NerToken(word='蕭美琴', ner='PERSON', idx=(468, 471))\n",
      "NerToken(word='推特', ner='GPE', idx=(475, 477))\n",
      "NerToken(word='台灣', ner='GPE', idx=(495, 497))\n",
      "NerToken(word='美方', ner='GPE', idx=(550, 552))\n",
      "NerToken(word='美國', ner='GPE', idx=(555, 557))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NerToken(word='台', ner='GPE', idx=(560, 561))\n",
      "NerToken(word='台', ner='GPE', idx=(580, 581))\n",
      "NerToken(word='日本', ner='GPE', idx=(613, 615))\n",
      "NerToken(word='立陶宛', ner='GPE', idx=(616, 619))\n",
      "NerToken(word='斯洛伐克', ner='GPE', idx=(620, 624))\n",
      "NerToken(word='捷克', ner='GPE', idx=(625, 627))\n",
      "NerToken(word='波蘭', ner='GPE', idx=(628, 630))\n",
      "NerToken(word='台灣', ner='GPE', idx=(634, 636))\n",
      "NerToken(word='中央社', ner='ORG', idx=(655, 658))\n",
      "NerToken(word='《TVBS》', ner='WORK_OF_ART', idx=(660, 666))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(672, 676))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(679, 682))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(715, 727))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623616\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 64.02it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.57s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國', ner='NORP', idx=(0, 2))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(4, 6))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(42, 45))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(50, 53))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(62, 65))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(68, 70))\n",
      "NerToken(word='白宮', ner='ORG', idx=(84, 86))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(87, 89))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(205, 208))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(210, 213))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(230, 233))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(244, 246))\n",
      "NerToken(word='國會山莊報', ner='ORG', idx=(264, 269))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(295, 298))\n",
      "NerToken(word='川普', ner='PERSON', idx=(319, 321))\n",
      "NerToken(word='2019冠狀病毒疾病', ner='EVENT', idx=(361, 371))\n",
      "NerToken(word='美國', ner='GPE', idx=(405, 407))\n",
      "NerToken(word='阿富汗', ner='GPE', idx=(428, 431))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(446, 448))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(498, 500))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(516, 519))\n",
      "NerToken(word='共和黨', ner='ORG', idx=(551, 554))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(557, 560))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(581, 584))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(597, 600))\n",
      "NerToken(word='共和黨', ner='ORG', idx=(610, 613))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(622, 625))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(629, 631))\n",
      "NerToken(word='民主黨', ner='ORG', idx=(638, 641))\n",
      "NerToken(word='共和黨', ner='ORG', idx=(653, 656))\n",
      "NerToken(word='川普', ner='PERSON', idx=(668, 670))\n",
      "NerToken(word='共和黨', ner='NORP', idx=(686, 689))\n",
      "NerToken(word='共和黨', ner='ORG', idx=(693, 696))\n",
      "NerToken(word='川普', ner='PERSON', idx=(706, 708))\n",
      "NerToken(word='共和黨', ner='ORG', idx=(710, 713))\n",
      "NerToken(word='美', ner='GPE', idx=(754, 755))\n",
      "NerToken(word='中央社', ner='ORG', idx=(780, 783))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623620\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.47s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.48s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='易淑寬', ner='PERSON', idx=(14, 17))\n",
      "NerToken(word='台灣靈異事件', ner='WORK_OF_ART', idx=(22, 28))\n",
      "NerToken(word='藍色水玲瓏', ner='WORK_OF_ART', idx=(31, 36))\n",
      "NerToken(word='戲說台灣', ner='WORK_OF_ART', idx=(39, 43))\n",
      "NerToken(word='易淑寬', ner='PERSON', idx=(111, 114))\n",
      "NerToken(word='易淑寬', ner='PERSON', idx=(192, 195))\n",
      "NerToken(word='易淑寬', ner='PERSON', idx=(276, 279))\n",
      "NerToken(word='易淑寬', ner='PERSON', idx=(335, 338))\n",
      "NerToken(word='全球商聯會', ner='ORG', idx=(352, 357))\n",
      "NerToken(word='易淑寬', ner='PERSON', idx=(420, 423))\n",
      "NerToken(word='越南', ner='GPE', idx=(433, 435))\n",
      "NerToken(word='泰國', ner='GPE', idx=(436, 438))\n",
      "NerToken(word='印尼', ner='GPE', idx=(439, 441))\n",
      "NerToken(word='大陸', ner='GPE', idx=(442, 444))\n",
      "NerToken(word='星馬', ner='GPE', idx=(445, 447))\n",
      "NerToken(word='台灣', ner='GPE', idx=(456, 458))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(534, 538))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(541, 544))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(577, 589))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623633\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.78s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.78s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.70s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.58s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='巴西', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='雷特', ner='PERSON', idx=(6, 8))\n",
      "NerToken(word='巴西', ner='GPE', idx=(13, 15))\n",
      "NerToken(word='聯合國', ner='ORG', idx=(48, 51))\n",
      "NerToken(word='COP26', ner='ORG', idx=(69, 74))\n",
      "NerToken(word='雷特（Joaquim Leite）', ner='PERSON', idx=(78, 95))\n",
      "NerToken(word='巴西利亞', ner='GPE', idx=(99, 103))\n",
      "NerToken(word='巴西', ner='GPE', idx=(118, 120))\n",
      "NerToken(word='巴西', ner='GPE', idx=(162, 164))\n",
      "NerToken(word='雷特', ner='PERSON', idx=(215, 217))\n",
      "NerToken(word='COP', ner='ORG', idx=(222, 225))\n",
      "NerToken(word='巴西', ner='GPE', idx=(290, 292))\n",
      "NerToken(word='波索納洛（Jair Bolsonaro）', ner='PERSON', idx=(294, 314))\n",
      "NerToken(word='蘇格蘭', ner='GPE', idx=(320, 323))\n",
      "NerToken(word='巴西', ner='GPE', idx=(378, 380))\n",
      "NerToken(word='巴西', ner='GPE', idx=(395, 397))\n",
      "NerToken(word='亞馬遜', ner='PERSON', idx=(441, 444))\n",
      "NerToken(word='巴西', ner='GPE', idx=(457, 459))\n",
      "NerToken(word='巴西', ner='GPE', idx=(472, 474))\n",
      "NerToken(word='巴西', ner='GPE', idx=(496, 498))\n",
      "NerToken(word='亞馬遜', ner='PERSON', idx=(530, 533))\n",
      "NerToken(word='波索納洛', ner='PERSON', idx=(552, 556))\n",
      "NerToken(word='巴西', ner='GPE', idx=(563, 565))\n",
      "NerToken(word='雷特', ner='PERSON', idx=(616, 618))\n",
      "NerToken(word='COP26會議', ner='EVENT', idx=(636, 643))\n",
      "NerToken(word='巴黎', ner='GPE', idx=(663, 665))\n",
      "NerToken(word='波索納洛', ner='PERSON', idx=(689, 693))\n",
      "NerToken(word='COP26', ner='ORG', idx=(695, 700))\n",
      "NerToken(word='巴西', ner='GPE', idx=(719, 721))\n",
      "NerToken(word='環境部長', ner='ORG', idx=(734, 738))\n",
      "NerToken(word='薩勒斯（Ricardo Salles）', ner='PERSON', idx=(738, 757))\n",
      "NerToken(word='巴西', ner='GPE', idx=(765, 767))\n",
      "NerToken(word='波索納洛', ner='GPE', idx=(788, 792))\n",
      "NerToken(word='巴西', ner='GPE', idx=(809, 811))\n",
      "NerToken(word='巴西', ner='GPE', idx=(831, 833))\n",
      "NerToken(word='阿斯特里尼', ner='PERSON', idx=(868, 873))\n",
      "NerToken(word='波索納洛', ner='PERSON', idx=(921, 925))\n",
      "NerToken(word='雷特', ner='PERSON', idx=(963, 965))\n",
      "NerToken(word='巴西', ner='GPE', idx=(980, 982))\n",
      "NerToken(word='中國大陸', ner='GPE', idx=(993, 997))\n",
      "NerToken(word='中國大陸', ner='GPE', idx=(1050, 1054))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NerToken(word='中央社', ner='ORG', idx=(1066, 1069))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623637\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.14s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.20s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.42s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.39s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.59it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.45s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 500.99it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 500.99it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='台灣', ner='GPE', idx=(107, 109))\n",
      "NerToken(word='凱撒大帝', ner='PERSON', idx=(159, 163))\n",
      "NerToken(word='拿破崙', ner='PERSON', idx=(164, 167))\n",
      "NerToken(word='牛頓', ner='PERSON', idx=(168, 170))\n",
      "NerToken(word='愛倫坡', ner='GPE', idx=(171, 174))\n",
      "NerToken(word='梵谷', ner='GPE', idx=(175, 177))\n",
      "NerToken(word='王毓禎', ner='PERSON', idx=(226, 229))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(232, 236))\n",
      "NerToken(word='王毓禎', ner='PERSON', idx=(308, 311))\n",
      "NerToken(word='中國大陸', ner='GPE', idx=(358, 362))\n",
      "NerToken(word='王毓禎', ner='PERSON', idx=(479, 482))\n",
      "NerToken(word='伊朗', ner='GPE', idx=(572, 574))\n",
      "NerToken(word='王毓禎', ner='PERSON', idx=(641, 644))\n",
      "NerToken(word='伊朗', ner='GPE', idx=(655, 657))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(672, 676))\n",
      "NerToken(word='王毓禎', ner='PERSON', idx=(726, 729))\n",
      "NerToken(word='台灣', ner='GPE', idx=(836, 838))\n",
      "NerToken(word='BNT', ner='PERSON', idx=(965, 968))\n",
      "NerToken(word='Moderna', ner='PERSON', idx=(983, 990))\n",
      "NerToken(word='AZ', ner='PERSON', idx=(1005, 1007))\n",
      "NerToken(word='J&J', ner='PERSON', idx=(1022, 1025))\n",
      "NerToken(word='王毓禎', ner='PERSON', idx=(1053, 1056))\n",
      "NerToken(word='台灣癲癇醫學會', ner='ORG', idx=(1068, 1075))\n",
      "NerToken(word='國際抗癲癇聯盟', ner='ORG', idx=(1076, 1083))\n",
      "NerToken(word='台市立聯合醫院', ner='ORG', idx=(1166, 1173))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(1212, 1217))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(1244, 1248))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(1251, 1254))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(1287, 1299))\n",
      "NerToken(word='HOT', ner='ORG', idx=(1301, 1304))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623638\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='小嫻', ner='PERSON', idx=(5, 7))\n",
      "NerToken(word='黃瑜嫻', ner='PERSON', idx=(8, 11))\n",
      "NerToken(word='小嫻', ner='PERSON', idx=(49, 51))\n",
      "NerToken(word='何守', ner='PERSON', idx=(106, 108))\n",
      "NerToken(word='\\n小嫻', ner='PERSON', idx=(122, 125))\n",
      "NerToken(word='何守正', ner='PERSON', idx=(131, 134))\n",
      "NerToken(word='小嫻', ner='PERSON', idx=(161, 163))\n",
      "NerToken(word='何守正', ner='PERSON', idx=(171, 174))\n",
      "NerToken(word='鏡週刊', ner='ORG', idx=(188, 191))\n",
      "NerToken(word='何守正', ner='PERSON', idx=(195, 198))\n",
      "NerToken(word='小嫻', ner='PERSON', idx=(213, 215))\n",
      "NerToken(word='台', ner='GPE', idx=(236, 237))\n",
      "NerToken(word='何守正', ner='PERSON', idx=(259, 262))\n",
      "NerToken(word='小嫻', ner='PERSON', idx=(266, 268))\n",
      "NerToken(word='何守正', ner='PERSON', idx=(306, 309))\n",
      "NerToken(word='何守正', ner='PERSON', idx=(387, 390))\n",
      "NerToken(word='何守正', ner='PERSON', idx=(437, 440))\n",
      "NerToken(word='何', ner='PERSON', idx=(474, 475))\n",
      "NerToken(word='郭彥均', ner='PERSON', idx=(508, 511))\n",
      "NerToken(word='張文綺', ner='PERSON', idx=(512, 515))\n",
      "NerToken(word='何守正', ner='PERSON', idx=(528, 531))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623642\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='張香香', ner='PERSON', idx=(19, 22))\n",
      "NerToken(word='張香香', ner='PERSON', idx=(71, 74))\n",
      "NerToken(word='IG', ner='ORG', idx=(78, 80))\n",
      "NerToken(word='張香香', ner='PERSON', idx=(149, 152))\n",
      "NerToken(word='比基尼', ner='GPE', idx=(189, 192))\n",
      "NerToken(word='張香香', ner='PERSON', idx=(273, 276))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623645\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.75s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.83s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='法新社', ner='ORG', idx=(2, 5))\n",
      "NerToken(word='中國', ner='GPE', idx=(16, 18))\n",
      "NerToken(word='COVID-19（2019冠狀病毒疾病', ner='EVENT', idx=(26, 45))\n",
      "NerToken(word='歐洲', ner='LOC', idx=(69, 71))\n",
      "NerToken(word='美媒', ner='NORP', idx=(129, 131))\n",
      "NerToken(word='法新社', ner='ORG', idx=(191, 194))\n",
      "NerToken(word='世界衛生組織（WHO）', ner='ORG', idx=(240, 251))\n",
      "NerToken(word='譚德塞', ner='PERSON', idx=(254, 257))\n",
      "NerToken(word='歐洲', ner='LOC', idx=(303, 305))\n",
      "NerToken(word='COVID-19', ner='ORG', idx=(310, 318))\n",
      "NerToken(word='歐洲區', ner='LOC', idx=(339, 342))\n",
      "NerToken(word='俄羅斯', ner='GPE', idx=(367, 370))\n",
      "NerToken(word='俄羅斯聯邦統計局', ner='ORG', idx=(468, 476))\n",
      "NerToken(word='Rosstat', ner='ORG', idx=(477, 484))\n",
      "NerToken(word='COVID-19', ner='ORG', idx=(487, 495))\n",
      "NerToken(word='烏克蘭', ner='GPE', idx=(520, 523))\n",
      "NerToken(word='羅馬尼亞', ner='GPE', idx=(524, 528))\n",
      "NerToken(word='歐洲', ner='LOC', idx=(529, 531))\n",
      "NerToken(word='俄羅斯', ner='GPE', idx=(542, 545))\n",
      "NerToken(word='拉丁美洲', ner='LOC', idx=(572, 576))\n",
      "NerToken(word='加勒比海', ner='LOC', idx=(577, 581))\n",
      "NerToken(word='美國', ner='GPE', idx=(638, 640))\n",
      "NerToken(word='美國', ner='GPE', idx=(669, 671))\n",
      "NerToken(word='COVID-19', ner='PRODUCT', idx=(703, 711))\n",
      "NerToken(word='中央社', ner='ORG', idx=(751, 754))\n",
      "NerToken(word='《TVBS》', ner='WORK_OF_ART', idx=(755, 761))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(767, 771))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(774, 777))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(810, 822))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623650\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.17s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.80s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.53it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.14s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.19s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 500.87it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.52s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.62s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='聯安診所', ner='ORG', idx=(115, 119))\n",
      "NerToken(word='施奕仲', ner='PERSON', idx=(127, 130))\n",
      "NerToken(word='施奕仲', ner='PERSON', idx=(418, 421))\n",
      "NerToken(word='施仲奕', ner='PERSON', idx=(654, 657))\n",
      "NerToken(word='施奕仲', ner='PERSON', idx=(842, 845))\n",
      "NerToken(word='施仲奕', ner='PERSON', idx=(957, 960))\n",
      "NerToken(word='施奕仲', ner='PERSON', idx=(1206, 1209))\n",
      "NerToken(word='聯安診所', ner='ORG', idx=(1408, 1412))\n",
      "NerToken(word='健康2.0', ner='WORK_OF_ART', idx=(1456, 1461))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623651\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='香港', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='黃秋生', ner='PERSON', idx=(4, 7))\n",
      "NerToken(word='台灣', ner='GPE', idx=(22, 24))\n",
      "NerToken(word='陳之漢', ner='PERSON', idx=(61, 64))\n",
      "NerToken(word='台灣', ner='GPE', idx=(80, 82))\n",
      "NerToken(word='台灣', ner='GPE', idx=(99, 101))\n",
      "NerToken(word='黃秋生', ner='PERSON', idx=(107, 110))\n",
      "NerToken(word='台灣', ner='GPE', idx=(117, 119))\n",
      "NerToken(word='陳美鳳', ner='PERSON', idx=(152, 155))\n",
      "NerToken(word='黃秋生', ner='PERSON', idx=(169, 172))\n",
      "NerToken(word='台灣', ner='GPE', idx=(174, 176))\n",
      "NerToken(word='台灣', ner='GPE', idx=(224, 226))\n",
      "NerToken(word='台灣', ner='GPE', idx=(246, 248))\n",
      "NerToken(word='You Know', ner='PERSON', idx=(268, 276))\n",
      "NerToken(word='香港', ner='GPE', idx=(277, 279))\n",
      "NerToken(word='台灣', ner='GPE', idx=(355, 357))\n",
      "NerToken(word='台灣人', ner='NORP', idx=(366, 369))\n",
      "NerToken(word='魷魚遊戲', ner='WORK_OF_ART', idx=(392, 396))\n",
      "NerToken(word='台灣', ner='GPE', idx=(410, 412))\n",
      "NerToken(word='黃秋生', ner='PERSON', idx=(446, 449))\n",
      "NerToken(word='台灣', ner='GPE', idx=(454, 456))\n",
      "NerToken(word='台灣人', ner='NORP', idx=(468, 471))\n",
      "NerToken(word='台灣', ner='GPE', idx=(495, 497))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623652\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.38s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.66it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.35s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='印尼', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='COVID-19', ner='PRODUCT', idx=(17, 25))\n",
      "NerToken(word='印尼', ner='GPE', idx=(28, 30))\n",
      "NerToken(word='中國', ner='GPE', idx=(46, 48))\n",
      "NerToken(word='中國', ner='GPE', idx=(58, 60))\n",
      "NerToken(word='美國', ner='GPE', idx=(63, 65))\n",
      "NerToken(word='輝瑞', ner='ORG', idx=(67, 69))\n",
      "NerToken(word='德國生技公司', ner='ORG', idx=(70, 76))\n",
      "NerToken(word='印尼食品藥物管理局（BPOM', ner='ORG', idx=(95, 109))\n",
      "NerToken(word='盧吉托（Penny Lukito）', ner='PERSON', idx=(112, 129))\n",
      "NerToken(word='中國科興', ner='ORG', idx=(149, 153))\n",
      "NerToken(word='印尼', ner='GPE', idx=(188, 190))\n",
      "NerToken(word='美國有線電視新聞網', ner='ORG', idx=(210, 219))\n",
      "NerToken(word='CNN Indonesia', ner='ORG', idx=(224, 237))\n",
      "NerToken(word='盧吉托', ner='PERSON', idx=(246, 249))\n",
      "NerToken(word='印尼衛生部', ner='ORG', idx=(327, 332))\n",
      "NerToken(word='席蒂', ner='PERSON', idx=(335, 337))\n",
      "NerToken(word='印尼衛生部', ner='ORG', idx=(408, 413))\n",
      "NerToken(word='科興', ner='ORG', idx=(418, 420))\n",
      "NerToken(word='印尼', ner='GPE', idx=(421, 423))\n",
      "NerToken(word='德國生技公司', ner='ORG', idx=(458, 464))\n",
      "NerToken(word='中央社', ner='ORG', idx=(493, 496))\n",
      "NerToken(word='《TVBS》', ner='WORK_OF_ART', idx=(497, 503))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(509, 513))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(516, 519))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(552, 564))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623655\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='韓國', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='文在寅', ner='PERSON', idx=(4, 7))\n",
      "NerToken(word='英國', ner='GPE', idx=(8, 10))\n",
      "NerToken(word='聯合國氣候變化綱要公約第26次締約方會議', ner='EVENT', idx=(16, 36))\n",
      "NerToken(word='韓國', ner='GPE', idx=(50, 52))\n",
      "NerToken(word='韓聯社', ner='ORG', idx=(89, 92))\n",
      "NerToken(word='文在寅', ner='PERSON', idx=(95, 98))\n",
      "NerToken(word='COP26', ner='PERSON', idx=(101, 106))\n",
      "NerToken(word='英國', ner='GPE', idx=(108, 110))\n",
      "NerToken(word='韓國', ner='GPE', idx=(131, 133))\n",
      "NerToken(word='文在寅', ner='PERSON', idx=(194, 197))\n",
      "NerToken(word='韓國', ner='GPE', idx=(235, 237))\n",
      "NerToken(word='美國', ner='GPE', idx=(279, 281))\n",
      "NerToken(word='歐盟', ner='ORG', idx=(282, 284))\n",
      "NerToken(word='文在寅', ner='PERSON', idx=(318, 321))\n",
      "NerToken(word='韓國', ner='GPE', idx=(343, 345))\n",
      "NerToken(word='二戰', ner='EVENT', idx=(348, 350))\n",
      "NerToken(word='文在寅', ner='PERSON', idx=(400, 403))\n",
      "NerToken(word='全球綠色成長研究所', ner='ORG', idx=(460, 469))\n",
      "NerToken(word='氣候技術中心', ner='ORG', idx=(471, 477))\n",
      "NerToken(word='文在寅', ner='PERSON', idx=(503, 506))\n",
      "NerToken(word='中央社', ner='ORG', idx=(543, 546))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623657\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.48s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:05<00:00,  5.18s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.35s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:04<00:00,  4.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.32s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.70s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='台裔', ner='NORP', idx=(0, 2))\n",
      "NerToken(word='波士頓市', ner='GPE', idx=(2, 6))\n",
      "NerToken(word='吳弭', ner='PERSON', idx=(8, 10))\n",
      "NerToken(word='波士頓', ner='GPE', idx=(76, 79))\n",
      "NerToken(word='波士頓', ner='GPE', idx=(158, 161))\n",
      "NerToken(word='吳弭', ner='PERSON', idx=(197, 199))\n",
      "NerToken(word='波士頓', ner='GPE', idx=(202, 205))\n",
      "NerToken(word='華埠', ner='FAC', idx=(205, 207))\n",
      "NerToken(word='波士頓人', ner='NORP', idx=(218, 222))\n",
      "NerToken(word='華埠', ner='FAC', idx=(228, 230))\n",
      "NerToken(word='華埠', ner='FAC', idx=(271, 273))\n",
      "NerToken(word='吳弭', ner='PERSON', idx=(279, 281))\n",
      "NerToken(word='華埠', ner='ORG', idx=(294, 296))\n",
      "NerToken(word='華埠', ner='ORG', idx=(352, 354))\n",
      "NerToken(word='吳弭', ner='PERSON', idx=(420, 422))\n",
      "NerToken(word='波士頓', ner='GPE', idx=(492, 495))\n",
      "NerToken(word='波士頓', ner='GPE', idx=(541, 544))\n",
      "NerToken(word='美', ner='GPE', idx=(579, 580))\n",
      "NerToken(word='吳弭', ner='PERSON', idx=(587, 589))\n",
      "NerToken(word='波士頓', ner='GPE', idx=(591, 594))\n",
      "NerToken(word='波士頓', ner='GPE', idx=(643, 646))\n",
      "NerToken(word='吳弭', ner='PERSON', idx=(657, 659))\n",
      "NerToken(word='波士頓', ner='GPE', idx=(677, 680))\n",
      "NerToken(word='台裔', ner='NORP', idx=(684, 686))\n",
      "NerToken(word='美籍', ner='NORP', idx=(686, 688))\n",
      "NerToken(word='吳弭', ner='PERSON', idx=(708, 710))\n",
      "NerToken(word='波士頓市', ner='GPE', idx=(718, 722))\n",
      "NerToken(word='議會', ner='ORG', idx=(722, 724))\n",
      "NerToken(word='美東', ner='LOC', idx=(795, 797))\n",
      "NerToken(word='波士頓', ner='GPE', idx=(844, 847))\n",
      "NerToken(word='吳弭', ner='PERSON', idx=(876, 878))\n",
      "NerToken(word='吳弭', ner='PERSON', idx=(942, 944))\n",
      "NerToken(word='台灣', ner='GPE', idx=(948, 950))\n",
      "NerToken(word='芝加哥', ner='GPE', idx=(952, 955))\n",
      "NerToken(word='哈佛大學', ner='ORG', idx=(961, 965))\n",
      "NerToken(word='波士頓', ner='GPE', idx=(970, 973))\n",
      "NerToken(word='哈佛大學', ner='ORG', idx=(975, 979))\n",
      "NerToken(word='芝加哥', ner='GPE', idx=(991, 994))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NerToken(word='波士頓', ner='GPE', idx=(1028, 1031))\n",
      "NerToken(word='華埠', ner='GPE', idx=(1031, 1033))\n",
      "NerToken(word='華人', ner='NORP', idx=(1039, 1041))\n",
      "NerToken(word='李衛新', ner='PERSON', idx=(1046, 1049))\n",
      "NerToken(word='吳弭', ner='PERSON', idx=(1054, 1056))\n",
      "NerToken(word='吳弭', ner='PERSON', idx=(1085, 1087))\n",
      "NerToken(word='波士頓', ner='GPE', idx=(1110, 1113))\n",
      "NerToken(word='李衛新', ner='PERSON', idx=(1121, 1124))\n",
      "NerToken(word='吳弭', ner='PERSON', idx=(1126, 1128))\n",
      "NerToken(word='中央社', ner='ORG', idx=(1183, 1186))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623665\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='江俊翰', ner='PERSON', idx=(8, 11))\n",
      "NerToken(word='廖文豪', ner='PERSON', idx=(14, 17))\n",
      "NerToken(word='江俊翰', ner='PERSON', idx=(70, 73))\n",
      "NerToken(word='江俊翰', ner='PERSON', idx=(87, 90))\n",
      "NerToken(word='花東', ner='GPE', idx=(95, 97))\n",
      "NerToken(word='江俊翰', ner='PERSON', idx=(164, 167))\n",
      "NerToken(word='花東', ner='GPE', idx=(199, 201))\n",
      "NerToken(word='江俊翰', ner='PERSON', idx=(318, 321))\n",
      "NerToken(word='TVBS', ner='WORK_OF_ART', idx=(608, 612))\n",
      "NerToken(word='《TVBS》', ner='WORK_OF_ART', idx=(635, 641))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(647, 651))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(654, 657))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(690, 702))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623667\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.38s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 63.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.66s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.32s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='COP', ner='ORG', idx=(21, 24))\n",
      "NerToken(word='英國', ner='GPE', idx=(45, 47))\n",
      "NerToken(word='法新社', ner='ORG', idx=(123, 126))\n",
      "NerToken(word='COP', ner='ORG', idx=(136, 139))\n",
      "NerToken(word='COP26大會', ner='EVENT', idx=(151, 158))\n",
      "NerToken(word='英國', ner='GPE', idx=(202, 204))\n",
      "NerToken(word='強生', ner='PERSON', idx=(206, 208))\n",
      "NerToken(word='英國', ner='GPE', idx=(264, 266))\n",
      "NerToken(word='強生', ner='PERSON', idx=(273, 275))\n",
      "NerToken(word='格拉斯哥', ner='GPE', idx=(278, 282))\n",
      "NerToken(word='強生', ner='PERSON', idx=(318, 320))\n",
      "NerToken(word='英國', ner='GPE', idx=(424, 426))\n",
      "NerToken(word='美國', ner='NORP', idx=(491, 493))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(495, 497))\n",
      "NerToken(word='巴西', ner='GPE', idx=(506, 508))\n",
      "NerToken(word='波索納洛', ner='PERSON', idx=(510, 514))\n",
      "NerToken(word='俄羅斯', ner='GPE', idx=(531, 534))\n",
      "NerToken(word='蒲亭', ner='PERSON', idx=(536, 538))\n",
      "NerToken(word='波索納洛', ner='GPE', idx=(561, 565))\n",
      "NerToken(word='蒲亭', ner='GPE', idx=(566, 568))\n",
      "NerToken(word='巴西', ner='GPE', idx=(579, 581))\n",
      "NerToken(word='印尼', ner='GPE', idx=(590, 592))\n",
      "NerToken(word='佐科威（Joko Widodo）', ner='PERSON', idx=(594, 610))\n",
      "NerToken(word='印尼', ner='GPE', idx=(623, 625))\n",
      "NerToken(word='英國', ner='GPE', idx=(645, 647))\n",
      "NerToken(word='佐科威', ner='GPE', idx=(653, 656))\n",
      "NerToken(word='綠色和平組織（Greenpeace）', ner='ORG', idx=(734, 752))\n",
      "NerToken(word='巴西', ner='GPE', idx=(784, 786))\n",
      "NerToken(word='帕斯奎爾（Carolina Pasquali', ner='PERSON', idx=(790, 812))\n",
      "NerToken(word='帕斯奎爾', ner='PERSON', idx=(858, 862))\n",
      "NerToken(word='中央社', ner='ORG', idx=(883, 886))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623669\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.95it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.16it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.03it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.22it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='台北市建管處', ner='ORG', idx=(7, 13))\n",
      "NerToken(word='陳', ner='PERSON', idx=(15, 16))\n",
      "NerToken(word='北檢', ner='ORG', idx=(36, 38))\n",
      "NerToken(word='調查局', ner='ORG', idx=(42, 45))\n",
      "NerToken(word='陳', ner='PERSON', idx=(47, 48))\n",
      "NerToken(word='北市', ner='GPE', idx=(74, 76))\n",
      "NerToken(word='台北市建築管理工程處', ner='ORG', idx=(96, 106))\n",
      "NerToken(word='建管處', ner='ORG', idx=(109, 112))\n",
      "NerToken(word='陳', ner='PERSON', idx=(112, 113))\n",
      "NerToken(word='台北地檢署', ner='ORG', idx=(146, 151))\n",
      "NerToken(word='陳', ner='PERSON', idx=(174, 175))\n",
      "NerToken(word='陳', ner='PERSON', idx=(192, 193))\n",
      "NerToken(word='中央社', ner='ORG', idx=(225, 228))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623670\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.98s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.96s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.95s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國西南航空（Southwest Airlines）', ner='ORG', idx=(0, 26))\n",
      "NerToken(word='布蘭登', ner='PERSON', idx=(59, 62))\n",
      "NerToken(word='美國', ner='GPE', idx=(101, 103))\n",
      "NerToken(word='西南航空', ner='ORG', idx=(152, 156))\n",
      "NerToken(word='布蘭登', ner='PERSON', idx=(201, 204))\n",
      "NerToken(word='美國', ner='GPE', idx=(237, 239))\n",
      "NerToken(word='阿拉巴馬州', ner='GPE', idx=(239, 244))\n",
      "NerToken(word='美國全國運動汽車競賽', ner='EVENT', idx=(256, 266))\n",
      "NerToken(word='NASCAR', ner='EVENT', idx=(267, 273))\n",
      "NerToken(word='納斯卡賽事', ner='EVENT', idx=(274, 279))\n",
      "NerToken(word='Xfinity系列賽', ner='EVENT', idx=(280, 290))\n",
      "NerToken(word='布蘭登‧布朗', ner='PERSON', idx=(303, 309))\n",
      "NerToken(word='美國國家廣播公司', ner='ORG', idx=(333, 341))\n",
      "NerToken(word='NBC', ner='ORG', idx=(343, 346))\n",
      "NerToken(word='NBC', ner='ORG', idx=(398, 401))\n",
      "NerToken(word='西南航空', ner='ORG', idx=(544, 548))\n",
      "NerToken(word='西南航空及公司', ner='ORG', idx=(601, 608))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623673\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='日本', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='馬小室圭', ner='PERSON', idx=(5, 9))\n",
      "NerToken(word='紐約', ner='GPE', idx=(14, 16))\n",
      "NerToken(word='日媒', ner='NORP', idx=(49, 51))\n",
      "NerToken(word='英文', ner='LANGUAGE', idx=(62, 64))\n",
      "NerToken(word='美國', ner='GPE', idx=(91, 93))\n",
      "NerToken(word='日本', ner='GPE', idx=(128, 130))\n",
      "NerToken(word='文仁', ner='PERSON', idx=(132, 134))\n",
      "NerToken(word='真子', ner='PERSON', idx=(138, 140))\n",
      "NerToken(word='小室圭', ner='PERSON', idx=(151, 154))\n",
      "NerToken(word='美國', ner='GPE', idx=(165, 167))\n",
      "NerToken(word='美國', ner='GPE', idx=(178, 180))\n",
      "NerToken(word='紐約', ner='GPE', idx=(185, 187))\n",
      "NerToken(word='NHK', ner='ORG', idx=(215, 218))\n",
      "NerToken(word='小室圭', ner='PERSON', idx=(221, 224))\n",
      "NerToken(word='日本', ner='GPE', idx=(227, 229))\n",
      "NerToken(word='律師事務所', ner='ORG', idx=(232, 237))\n",
      "NerToken(word='奧野善彥', ner='PERSON', idx=(239, 243))\n",
      "NerToken(word='小室圭', ner='PERSON', idx=(246, 249))\n",
      "NerToken(word='富士新聞網', ner='ORG', idx=(288, 293))\n",
      "NerToken(word='紐約州', ner='GPE', idx=(297, 300))\n",
      "NerToken(word='Ricky德永', ner='PERSON', idx=(302, 309))\n",
      "NerToken(word='小室圭', ner='GPE', idx=(312, 315))\n",
      "NerToken(word='英語', ner='LANGUAGE', idx=(323, 325))\n",
      "NerToken(word='日本大學的法學部', ner='ORG', idx=(340, 348))\n",
      "NerToken(word='美國', ner='GPE', idx=(354, 356))\n",
      "NerToken(word='英語', ner='LANGUAGE', idx=(362, 364))\n",
      "NerToken(word='美國', ner='GPE', idx=(370, 372))\n",
      "NerToken(word='日本', ner='GPE', idx=(394, 396))\n",
      "NerToken(word='小室圭', ner='PERSON', idx=(412, 415))\n",
      "NerToken(word='美國律師事務所', ner='ORG', idx=(423, 430))\n",
      "NerToken(word='紐約', ner='GPE', idx=(592, 594))\n",
      "NerToken(word='口一磨', ner='PERSON', idx=(599, 602))\n",
      "NerToken(word='小室圭', ner='PERSON', idx=(606, 609))\n",
      "NerToken(word='美國', ner='GPE', idx=(621, 623))\n",
      "NerToken(word='小室圭', ner='PERSON', idx=(721, 724))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623674\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.53it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='新北市保大', ner='ORG', idx=(12, 17))\n",
      "NerToken(word='三重區中興橋', ner='FAC', idx=(20, 26))\n",
      "NerToken(word='西門町', ner='LOC', idx=(98, 101))\n",
      "NerToken(word='林', ner='PERSON', idx=(234, 235))\n",
      "NerToken(word='林男', ner='PERSON', idx=(311, 313))\n",
      "NerToken(word='西門町', ner='GPE', idx=(367, 370))\n",
      "NerToken(word='林男', ner='PERSON', idx=(435, 437))\n",
      "NerToken(word='胡睿兒', ner='PERSON', idx=(447, 450))\n",
      "NerToken(word='林', ner='PERSON', idx=(567, 568))\n",
      "NerToken(word='毒品危害防制條例', ner='LAW', idx=(609, 617))\n",
      "NerToken(word='新北地檢', ner='ORG', idx=(621, 625))\n",
      "NerToken(word='TVBS', ner='WORK_OF_ART', idx=(642, 646))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623685\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.74s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.71s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.60s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.52s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='英國', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='伊麗莎白二世（Queen Elizabeth II）', ner='PERSON', idx=(4, 30))\n",
      "NerToken(word='法新社', ner='ORG', idx=(71, 74))\n",
      "NerToken(word='蘇格蘭格拉斯哥', ner='GPE', idx=(89, 96))\n",
      "NerToken(word='聯合國', ner='ORG', idx=(433, 436))\n",
      "NerToken(word='古特瑞斯', ner='PERSON', idx=(439, 443))\n",
      "NerToken(word='路透社', ner='ORG', idx=(554, 557))\n",
      "NerToken(word='格拉斯哥', ner='GPE', idx=(569, 573))\n",
      "NerToken(word='溫莎城堡', ner='GPE', idx=(647, 651))\n",
      "NerToken(word='中央社', ner='ORG', idx=(657, 660))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623687\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國', ner='NORP', idx=(14, 16))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(18, 20))\n",
      "NerToken(word='澳洲', ner='GPE', idx=(50, 52))\n",
      "NerToken(word='艾伯特', ner='PERSON', idx=(54, 57))\n",
      "NerToken(word='澳洲', ner='GPE', idx=(63, 65))\n",
      "NerToken(word='大陸', ner='GPE', idx=(72, 74))\n",
      "NerToken(word='臺灣', ner='GPE', idx=(90, 92))\n",
      "NerToken(word='北京', ner='GPE', idx=(100, 102))\n",
      "NerToken(word='澳洲', ner='GPE', idx=(117, 119))\n",
      "NerToken(word='艾伯特', ner='PERSON', idx=(121, 124))\n",
      "NerToken(word='中國', ner='GPE', idx=(126, 128))\n",
      "NerToken(word='臺灣', ner='GPE', idx=(137, 139))\n",
      "NerToken(word='中國', ner='GPE', idx=(185, 187))\n",
      "NerToken(word='澳洲', ner='GPE', idx=(221, 223))\n",
      "NerToken(word='艾伯特', ner='PERSON', idx=(225, 228))\n",
      "NerToken(word='臺灣', ner='GPE', idx=(235, 237))\n",
      "NerToken(word='艾伯特', ner='PERSON', idx=(253, 256))\n",
      "NerToken(word='澳洲', ner='GPE', idx=(263, 265))\n",
      "NerToken(word='大陸', ner='GPE', idx=(266, 268))\n",
      "NerToken(word='大陸', ner='GPE', idx=(281, 283))\n",
      "NerToken(word='習近平', ner='PERSON', idx=(287, 290))\n",
      "NerToken(word='國會', ner='ORG', idx=(291, 293))\n",
      "NerToken(word='艾伯特', ner='PERSON', idx=(296, 299))\n",
      "NerToken(word='大陸', ner='GPE', idx=(302, 304))\n",
      "NerToken(word='艾伯特', ner='PERSON', idx=(319, 322))\n",
      "NerToken(word='北京', ner='GPE', idx=(325, 327))\n",
      "NerToken(word='澳洲', ner='GPE', idx=(338, 340))\n",
      "NerToken(word='澳洲', ner='GPE', idx=(352, 354))\n",
      "NerToken(word='大陸', ner='GPE', idx=(362, 364))\n",
      "NerToken(word='澳洲', ner='GPE', idx=(369, 371))\n",
      "NerToken(word='艾伯特', ner='PERSON', idx=(373, 376))\n",
      "NerToken(word='中國', ner='GPE', idx=(386, 388))\n",
      "NerToken(word='中國', ner='GPE', idx=(417, 419))\n",
      "NerToken(word='G20峰會', ner='EVENT', idx=(446, 451))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(454, 456))\n",
      "NerToken(word='歐盟', ner='ORG', idx=(462, 464))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(486, 488))\n",
      "NerToken(word='大陸', ner='GPE', idx=(514, 516))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NerToken(word='美國', ner='NORP', idx=(535, 537))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(539, 541))\n",
      "NerToken(word='大陸', ner='GPE', idx=(631, 633))\n",
      "NerToken(word='兩岸', ner='GPE', idx=(636, 638))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623690\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.88s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='亞歷鮑德溫', ner='PERSON', idx=(2, 7))\n",
      "NerToken(word='賀勤茲', ner='PERSON', idx=(16, 19))\n",
      "NerToken(word='亞歷鮑德溫', ner='PERSON', idx=(26, 31))\n",
      "NerToken(word='郝斯', ner='PERSON', idx=(36, 38))\n",
      "NerToken(word='亞歷鮑德溫', ner='PERSON', idx=(71, 76))\n",
      "NerToken(word='新墨西哥州', ner='GPE', idx=(93, 98))\n",
      "NerToken(word='賀勤茲', ner='PERSON', idx=(102, 105))\n",
      "NerToken(word='郝斯', ner='PERSON', idx=(157, 159))\n",
      "NerToken(word='紐約郵報', ner='ORG', idx=(164, 168))\n",
      "NerToken(word='賀勤茲', ner='PERSON', idx=(192, 195))\n",
      "NerToken(word='郝斯', ner='PERSON', idx=(226, 228))\n",
      "NerToken(word='賀勤茲', ner='PERSON', idx=(232, 235))\n",
      "NerToken(word='郝斯', ner='PERSON', idx=(297, 299))\n",
      "NerToken(word='古提瑞茲－利德', ner='PERSON', idx=(303, 310))\n",
      "NerToken(word='亞歷鮑德溫', ner='PERSON', idx=(352, 357))\n",
      "NerToken(word='郝斯', ner='PERSON', idx=(385, 387))\n",
      "NerToken(word='亞歷鮑德溫', ner='PERSON', idx=(414, 419))\n",
      "NerToken(word='賀勤茲', ner='PERSON', idx=(437, 440))\n",
      "NerToken(word='蘇薩', ner='PERSON', idx=(443, 445))\n",
      "NerToken(word='中央社', ner='ORG', idx=(465, 468))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623693\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='蘇格蘭', ner='GPE', idx=(30, 33))\n",
      "NerToken(word='COP26氣候峰會', ner='EVENT', idx=(35, 44))\n",
      "NerToken(word='美國', ner='NORP', idx=(65, 67))\n",
      "NerToken(word='拜登（Joe Biden）', ner='PERSON', idx=(69, 82))\n",
      "NerToken(word='川普（Donald Trump）', ner='PERSON', idx=(105, 121))\n",
      "NerToken(word='華盛頓郵報', ner='ORG', idx=(155, 160))\n",
      "NerToken(word='南非', ner='GPE', idx=(171, 173))\n",
      "NerToken(word='恩多普', ner='PERSON', idx=(176, 179))\n",
      "NerToken(word='Eddie Ndopu', ner='PERSON', idx=(180, 191))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(287, 289))\n",
      "NerToken(word='川普', ner='PERSON', idx=(319, 321))\n",
      "NerToken(word='歐洲', ner='LOC', idx=(328, 330))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(392, 394))\n",
      "NerToken(word='美國', ner='GPE', idx=(395, 397))\n",
      "NerToken(word='美國', ner='GPE', idx=(413, 415))\n",
      "NerToken(word='拜登', ner='PERSON', idx=(420, 422))\n",
      "NerToken(word='川普', ner='PERSON', idx=(440, 442))\n",
      "NerToken(word='歐洲', ner='LOC', idx=(488, 490))\n",
      "NerToken(word='COP26氣候峰會', ner='EVENT', idx=(503, 512))\n",
      "NerToken(word='羅馬', ner='GPE', idx=(522, 524))\n",
      "NerToken(word='G20', ner='EVENT', idx=(526, 529))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623695\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.98s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.50s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.29s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.59it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='中國大陸', ner='GPE', idx=(6, 10))\n",
      "NerToken(word='中國', ner='GPE', idx=(49, 51))\n",
      "NerToken(word='勞動法', ner='LAW', idx=(52, 55))\n",
      "NerToken(word='中國', ner='NORP', idx=(111, 113))\n",
      "NerToken(word='《南華早報》', ner='ORG', idx=(163, 169))\n",
      "NerToken(word='北京', ner='NORP', idx=(178, 180))\n",
      "NerToken(word='范玉軒', ner='PERSON', idx=(182, 185))\n",
      "NerToken(word='Fan Yuxuan', ner='PERSON', idx=(188, 198))\n",
      "NerToken(word='范', ner='PERSON', idx=(342, 343))\n",
      "NerToken(word='北京', ner='GPE', idx=(428, 430))\n",
      "NerToken(word='中國', ner='GPE', idx=(485, 487))\n",
      "NerToken(word='范', ner='PERSON', idx=(495, 496))\n",
      "NerToken(word='中國', ner='NORP', idx=(504, 506))\n",
      "NerToken(word='習近平', ner='PERSON', idx=(578, 581))\n",
      "NerToken(word='中國', ner='GPE', idx=(594, 596))\n",
      "NerToken(word='中國', ner='GPE', idx=(635, 637))\n",
      "NerToken(word='中國', ner='GPE', idx=(654, 656))\n",
      "NerToken(word='德國', ner='GPE', idx=(675, 677))\n",
      "NerToken(word='中國大陸', ner='GPE', idx=(681, 685))\n",
      "NerToken(word='中國', ner='GPE', idx=(691, 693))\n",
      "NerToken(word='中國', ner='GPE', idx=(718, 720))\n",
      "NerToken(word='中國', ner='GPE', idx=(732, 734))\n",
      "NerToken(word='中國', ner='NORP', idx=(793, 795))\n",
      "NerToken(word='中國', ner='GPE', idx=(813, 815))\n",
      "NerToken(word='\\xa0\\xa0 \\xa0\\n\\xa0', ner='ORG', idx=(845, 851))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623696\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.41s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='賈靜雯', ner='PERSON', idx=(1, 4))\n",
      "NerToken(word='王淨', ner='PERSON', idx=(5, 7))\n",
      "NerToken(word='瀑布', ner='WORK_OF_ART', idx=(15, 17))\n",
      "NerToken(word='鍾孟宏', ner='PERSON', idx=(22, 25))\n",
      "NerToken(word='金馬獎', ner='WORK_OF_ART', idx=(41, 44))\n",
      "NerToken(word='台灣', ner='GPE', idx=(54, 56))\n",
      "NerToken(word='賈靜雯', ner='PERSON', idx=(82, 85))\n",
      "NerToken(word='賈靜雯', ner='PERSON', idx=(116, 119))\n",
      "NerToken(word='瀑布', ner='WORK_OF_ART', idx=(179, 181))\n",
      "NerToken(word='賈靜雯', ner='PERSON', idx=(194, 197))\n",
      "NerToken(word='賈靜雯', ner='PERSON', idx=(245, 248))\n",
      "NerToken(word='賈靜雯', ner='PERSON', idx=(302, 305))\n",
      "NerToken(word='瀑布', ner='WORK_OF_ART', idx=(391, 393))\n",
      "NerToken(word='瀑布', ner='WORK_OF_ART', idx=(450, 452))\n",
      "NerToken(word='台', ner='GPE', idx=(467, 468))\n",
      "NerToken(word='賈靜雯', ner='PERSON', idx=(476, 479))\n",
      "NerToken(word='納豆', ner='PERSON', idx=(537, 539))\n",
      "NerToken(word='賈靜雯', ner='PERSON', idx=(549, 552))\n",
      "NerToken(word='鍾孟宏', ner='PERSON', idx=(581, 584))\n",
      "NerToken(word='賈靜雯', ner='PERSON', idx=(586, 589))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623716\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.99s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.53s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='郝根', ner='PERSON', idx=(18, 20))\n",
      "NerToken(word='祖克柏', ner='PERSON', idx=(51, 54))\n",
      "NerToken(word='郝根', ner='PERSON', idx=(79, 81))\n",
      "NerToken(word='葡萄牙', ner='GPE', idx=(84, 87))\n",
      "NerToken(word='里斯本', ner='GPE', idx=(87, 90))\n",
      "NerToken(word='祖克柏', ner='PERSON', idx=(123, 126))\n",
      "NerToken(word='郝根', ner='PERSON', idx=(149, 151))\n",
      "NerToken(word='Meta', ner='ORG', idx=(259, 263))\n",
      "NerToken(word='Meta', ner='PERSON', idx=(295, 299))\n",
      "NerToken(word='路透社', ner='ORG', idx=(322, 325))\n",
      "NerToken(word='郝根', ner='PERSON', idx=(384, 386))\n",
      "NerToken(word='國會', ner='ORG', idx=(485, 487))\n",
      "NerToken(word='郝根', ner='PERSON', idx=(500, 502))\n",
      "NerToken(word='英國', ner='GPE', idx=(506, 508))\n",
      "NerToken(word='美國', ner='GPE', idx=(509, 511))\n",
      "NerToken(word='國會', ner='ORG', idx=(511, 513))\n",
      "NerToken(word='郝根', ner='PERSON', idx=(581, 583))\n",
      "NerToken(word='郝根', ner='PERSON', idx=(640, 642))\n",
      "NerToken(word='中央社', ner='ORG', idx=(661, 664))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623719\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.28s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='中國', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='習近平', ner='PERSON', idx=(6, 9))\n",
      "NerToken(word='COP26峰會', ner='EVENT', idx=(85, 92))\n",
      "NerToken(word='英國', ner='GPE', idx=(96, 98))\n",
      "NerToken(word='格拉斯哥', ner='GPE', idx=(98, 102))\n",
      "NerToken(word='中國', ner='GPE', idx=(137, 139))\n",
      "NerToken(word='習近平', ner='PERSON', idx=(143, 146))\n",
      "NerToken(word='俄羅斯', ner='GPE', idx=(180, 183))\n",
      "NerToken(word='普欽（Vladimir Putin', ner='PERSON', idx=(185, 202))\n",
      "NerToken(word='路透社', ner='ORG', idx=(210, 213))\n",
      "NerToken(word='中國', ner='GPE', idx=(230, 232))\n",
      "NerToken(word='新華社', ner='ORG', idx=(234, 237))\n",
      "NerToken(word='習近平', ner='PERSON', idx=(240, 243))\n",
      "NerToken(word='聯合國氣候變化綱要公約', ner='LAW', idx=(276, 287))\n",
      "NerToken(word='巴黎協定', ner='LAW', idx=(290, 294))\n",
      "NerToken(word='習近平', ner='PERSON', idx=(422, 425))\n",
      "NerToken(word='中國', ner='GPE', idx=(428, 430))\n",
      "NerToken(word='中央社', ner='ORG', idx=(487, 490))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623721\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.11it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.39s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.36s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='彰化', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='彰化市', ner='GPE', idx=(194, 197))\n",
      "NerToken(word='中山路', ner='LOC', idx=(197, 200))\n",
      "NerToken(word='光復路', ner='LOC', idx=(201, 204))\n",
      "NerToken(word='彰化地檢署', ner='ORG', idx=(536, 541))\n",
      "NerToken(word='TVBS', ner='WORK_OF_ART', idx=(545, 549))\n",
      "NerToken(word='法律扶助基金會', ner='ORG', idx=(618, 625))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623726\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='高雄', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='鹽埕區', ner='LOC', idx=(8, 11))\n",
      "NerToken(word='許', ner='PERSON', idx=(480, 481))\n",
      "NerToken(word='鹽埕區', ner='LOC', idx=(486, 489))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623727\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.66s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.66s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='SpaceX', ner='ORG', idx=(10, 16))\n",
      "NerToken(word='特斯拉', ner='PERSON', idx=(21, 24))\n",
      "NerToken(word='馬斯克（Elon Musk', ner='PERSON', idx=(34, 47))\n",
      "NerToken(word='中文', ner='LANGUAGE', idx=(63, 65))\n",
      "NerToken(word='曹植', ner='PERSON', idx=(67, 69))\n",
      "NerToken(word='七步詩', ner='WORK_OF_ART', idx=(71, 74))\n",
      "NerToken(word='英文', ner='LANGUAGE', idx=(80, 82))\n",
      "NerToken(word='霧薩薩', ner='PRODUCT', idx=(132, 135))\n",
      "NerToken(word='馬斯克', ner='PERSON', idx=(141, 144))\n",
      "NerToken(word='大陸', ner='GPE', idx=(155, 157))\n",
      "NerToken(word='環時網', ner='ORG', idx=(158, 161))\n",
      "NerToken(word='馬斯克', ner='PERSON', idx=(184, 187))\n",
      "NerToken(word='中文', ner='LANGUAGE', idx=(188, 190))\n",
      "NerToken(word='馬斯克', ner='GPE', idx=(256, 259))\n",
      "NerToken(word='Humankind', ner='WORK_OF_ART', idx=(359, 368))\n",
      "NerToken(word='馬斯克', ner='PERSON', idx=(379, 382))\n",
      "NerToken(word='富比士（Forbes）', ner='ORG', idx=(397, 408))\n",
      "NerToken(word='馬斯克', ner='GPE', idx=(410, 413))\n",
      "NerToken(word='聯合國世界糧食計畫署', ner='ORG', idx=(458, 468))\n",
      "NerToken(word='WFP', ner='ORG', idx=(469, 472))\n",
      "NerToken(word='馬斯克', ner='GPE', idx=(480, 483))\n",
      "NerToken(word='馬斯克', ner='PERSON', idx=(515, 518))\n",
      "NerToken(word='特斯拉', ner='PERSON', idx=(557, 560))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623731\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.00s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.46s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='歐漢聲', ner='PERSON', idx=(8, 11))\n",
      "NerToken(word='大陸', ner='GPE', idx=(16, 18))\n",
      "NerToken(word='鄭雲燦', ner='PERSON', idx=(20, 23))\n",
      "NerToken(word='鄭雲燦', ner='PERSON', idx=(46, 49))\n",
      "NerToken(word='陳昊森', ner='PERSON', idx=(69, 72))\n",
      "NerToken(word='吳宗憲', ner='PERSON', idx=(106, 109))\n",
      "NerToken(word='Makiyo', ner='PERSON', idx=(174, 180))\n",
      "NerToken(word='大陸', ner='GPE', idx=(200, 202))\n",
      "NerToken(word='鄭雲燦', ner='PERSON', idx=(204, 207))\n",
      "NerToken(word='小嫻', ner='PERSON', idx=(302, 304))\n",
      "NerToken(word='加護', ner='GPE', idx=(322, 324))\n",
      "NerToken(word='鏡週刊', ner='ORG', idx=(327, 330))\n",
      "NerToken(word='鄭雲燦', ner='PERSON', idx=(349, 352))\n",
      "NerToken(word='刻在你心底的名字', ner='WORK_OF_ART', idx=(363, 371))\n",
      "NerToken(word='陳昊森', ner='PERSON', idx=(382, 385))\n",
      "NerToken(word='鄭雲燦', ner='PERSON', idx=(398, 401))\n",
      "NerToken(word='印度', ner='GPE', idx=(435, 437))\n",
      "NerToken(word='鄭雲燦', ner='PERSON', idx=(455, 458))\n",
      "NerToken(word='吳宗憲', ner='PERSON', idx=(492, 495))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623732\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='日本', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='日本', ner='GPE', idx=(31, 33))\n",
      "NerToken(word='NHK', ner='ORG', idx=(105, 108))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(113, 117))\n",
      "NerToken(word='日本', ner='GPE', idx=(123, 125))\n",
      "NerToken(word='日本', ner='GPE', idx=(178, 180))\n",
      "NerToken(word='日本', ner='GPE', idx=(203, 205))\n",
      "NerToken(word='日本', ner='GPE', idx=(226, 228))\n",
      "NerToken(word='輝瑞BNT', ner='PRODUCT', idx=(300, 305))\n",
      "NerToken(word='莫德納', ner='PRODUCT', idx=(306, 309))\n",
      "NerToken(word='AZ', ner='PRODUCT', idx=(310, 312))\n",
      "NerToken(word='PCR', ner='ORG', idx=(333, 336))\n",
      "NerToken(word='日本', ner='GPE', idx=(380, 382))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623735\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='馬來西亞', ner='GPE', idx=(0, 4))\n",
      "NerToken(word='黃明志', ner='PERSON', idx=(6, 9))\n",
      "NerToken(word='陳芳語', ner='PERSON', idx=(14, 17))\n",
      "NerToken(word='玻璃心', ner='WORK_OF_ART', idx=(23, 26))\n",
      "NerToken(word='台灣', ner='GPE', idx=(48, 50))\n",
      "NerToken(word='香港', ner='GPE', idx=(51, 53))\n",
      "NerToken(word='新加坡', ner='GPE', idx=(54, 57))\n",
      "NerToken(word='馬來西亞', ner='GPE', idx=(58, 62))\n",
      "NerToken(word='YouTube', ner='ORG', idx=(74, 81))\n",
      "NerToken(word='中文', ner='LANGUAGE', idx=(97, 99))\n",
      "NerToken(word='黃明志', ner='PERSON', idx=(107, 110))\n",
      "NerToken(word='周杰倫', ner='PERSON', idx=(126, 129))\n",
      "NerToken(word='黃明志', ner='PERSON', idx=(133, 136))\n",
      "NerToken(word='周杰倫', ner='PERSON', idx=(145, 148))\n",
      "NerToken(word='周董', ner='PERSON', idx=(162, 164))\n",
      "NerToken(word='黃秋生', ner='PERSON', idx=(177, 180))\n",
      "NerToken(word='台灣', ner='GPE', idx=(181, 183))\n",
      "NerToken(word='黃明志', ner='PERSON', idx=(214, 217))\n",
      "NerToken(word='周杰倫', ner='PERSON', idx=(222, 225))\n",
      "NerToken(word='玻璃心', ner='WORK_OF_ART', idx=(239, 242))\n",
      "NerToken(word='黃明志', ner='PERSON', idx=(257, 260))\n",
      "NerToken(word='周杰倫', ner='PERSON', idx=(296, 299))\n",
      "NerToken(word='PO', ner='PERSON', idx=(322, 324))\n",
      "NerToken(word='周杰倫', ner='PERSON', idx=(334, 337))\n",
      "NerToken(word='華語', ner='LANGUAGE', idx=(339, 341))\n",
      "NerToken(word='黃明志', ner='PERSON', idx=(361, 364))\n",
      "NerToken(word='黃', ner='PERSON', idx=(380, 381))\n",
      "NerToken(word='周杰倫', ner='PERSON', idx=(395, 398))\n",
      "NerToken(word='黃志明', ner='PERSON', idx=(406, 409))\n",
      "NerToken(word='陳美鳳', ner='PERSON', idx=(436, 439))\n",
      "NerToken(word='港星', ner='NORP', idx=(449, 451))\n",
      "NerToken(word='周杰倫', ner='PERSON', idx=(459, 462))\n",
      "NerToken(word='黃明志', ner='PERSON', idx=(465, 468))\n",
      "NerToken(word='周杰倫', ner='PERSON', idx=(479, 482))\n",
      "NerToken(word='馬來西亞', ner='GPE', idx=(511, 515))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623742\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.89s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.86s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='台中台灣大道', ner='FAC', idx=(0, 6))\n",
      "NerToken(word='台中', ner='GPE', idx=(230, 232))\n",
      "NerToken(word='台灣大道二段忠明南路口', ner='FAC', idx=(233, 244))\n",
      "NerToken(word='BMW', ner='NORP', idx=(380, 383))\n",
      "NerToken(word='第一分局', ner='ORG', idx=(418, 422))\n",
      "NerToken(word='周宜瑩', ner='PERSON', idx=(427, 430))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623745\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.04s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.02s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.05s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.27s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='舊金山灣區', ner='LOC', idx=(0, 5))\n",
      "NerToken(word='舊金山', ner='GPE', idx=(28, 31))\n",
      "NerToken(word='Meta', ner='ORG', idx=(47, 51))\n",
      "NerToken(word='舊金山紀事報', ner='ORG', idx=(79, 85))\n",
      "NerToken(word='San Francisco Chronicle', ner='ORG', idx=(87, 110))\n",
      "NerToken(word='波士頓地產', ner='ORG', idx=(116, 121))\n",
      "NerToken(word='湯瑪斯', ner='PERSON', idx=(143, 146))\n",
      "NerToken(word='TikTok母公司', ner='ORG', idx=(193, 202))\n",
      "NerToken(word='Meta', ner='ORG', idx=(379, 383))\n",
      "NerToken(word='COVID-19', ner='EVENT', idx=(437, 445))\n",
      "NerToken(word='舊金山', ner='GPE', idx=(466, 469))\n",
      "NerToken(word='東灣', ner='LOC', idx=(469, 471))\n",
      "NerToken(word='佛利蒙特（Fremont）', ner='ORG', idx=(472, 485))\n",
      "NerToken(word='湯瑪斯', ner='PERSON', idx=(491, 494))\n",
      "NerToken(word='西雅圖', ner='GPE', idx=(504, 507))\n",
      "NerToken(word='亞馬遜（Amazon）', ner='PERSON', idx=(524, 535))\n",
      "NerToken(word='亞馬遜', ner='PERSON', idx=(541, 544))\n",
      "NerToken(word='南聯合湖區', ner='LOC', idx=(549, 554))\n",
      "NerToken(word='TikTok', ner='ORG', idx=(589, 595))\n",
      "NerToken(word='波士頓地產', ner='ORG', idx=(646, 651))\n",
      "NerToken(word='舊金山灣區', ner='LOC', idx=(659, 664))\n",
      "NerToken(word='美', ner='GPE', idx=(688, 689))\n",
      "NerToken(word='紐約', ner='GPE', idx=(693, 695))\n",
      "NerToken(word='德州', ner='GPE', idx=(710, 712))\n",
      "NerToken(word='奧斯汀', ner='GPE', idx=(712, 715))\n",
      "NerToken(word='湯瑪斯', ner='PERSON', idx=(722, 725))\n",
      "NerToken(word='舊金山', ner='GPE', idx=(728, 731))\n",
      "NerToken(word='美', ner='GPE', idx=(744, 745))\n",
      "NerToken(word='舊金山', ner='GPE', idx=(757, 760))\n",
      "NerToken(word='舊金山', ner='GPE', idx=(772, 775))\n",
      "NerToken(word='中央社', ner='ORG', idx=(787, 790))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623748\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.06it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.09it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.13it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='桃園市', ner='GPE', idx=(0, 3))\n",
      "NerToken(word='陳', ner='PERSON', idx=(6, 7))\n",
      "NerToken(word='復興區', ner='LOC', idx=(16, 19))\n",
      "NerToken(word='大漢溪', ner='LOC', idx=(19, 22))\n",
      "NerToken(word='內政部空中勤務總隊', ner='ORG', idx=(34, 43))\n",
      "NerToken(word='郭百倫', ner='PERSON', idx=(93, 96))\n",
      "NerToken(word='中央社', ner='ORG', idx=(100, 103))\n",
      "NerToken(word='大漢溪', ner='LOC', idx=(119, 122))\n",
      "NerToken(word='陳', ner='PERSON', idx=(126, 127))\n",
      "NerToken(word='郭百倫', ner='PERSON', idx=(142, 145))\n",
      "NerToken(word='陳', ner='PERSON', idx=(184, 185))\n",
      "NerToken(word='郭百倫', ner='PERSON', idx=(201, 204))\n",
      "NerToken(word='郭百倫', ner='PERSON', idx=(272, 275))\n",
      "NerToken(word='陳', ner='PERSON', idx=(298, 299))\n",
      "NerToken(word='陳', ner='PERSON', idx=(307, 308))\n",
      "NerToken(word='中央社', ner='ORG', idx=(346, 349))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623752\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='奈及利亞', ner='GPE', idx=(0, 4))\n",
      "NerToken(word='拉哥斯', ner='GPE', idx=(8, 11))\n",
      "NerToken(word='泰特', ner='PERSON', idx=(155, 157))\n",
      "NerToken(word='奈及利亞', ner='GPE', idx=(230, 234))\n",
      "NerToken(word='拉哥斯市', ner='GPE', idx=(239, 243))\n",
      "NerToken(word='艾克潘', ner='PERSON', idx=(347, 350))\n",
      "NerToken(word='路透社', ner='ORG', idx=(394, 397))\n",
      "NerToken(word='約翰', ner='PERSON', idx=(461, 463))\n",
      "NerToken(word='奈及利亞', ner='GPE', idx=(495, 499))\n",
      "NerToken(word='拉哥斯', ner='GPE', idx=(535, 538))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623753\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='歐漢聲', ner='PERSON', idx=(2, 5))\n",
      "NerToken(word='大陸', ner='GPE', idx=(17, 19))\n",
      "NerToken(word='鄭雲燦', ner='PERSON', idx=(21, 24))\n",
      "NerToken(word='鄭雲燦', ner='PERSON', idx=(48, 51))\n",
      "NerToken(word='陳昊森', ner='PERSON', idx=(61, 64))\n",
      "NerToken(word='鏡週刊', ner='ORG', idx=(132, 135))\n",
      "NerToken(word='刻在你心底的名字', ner='WORK_OF_ART', idx=(143, 151))\n",
      "NerToken(word='陳昊森', ner='PERSON', idx=(160, 163))\n",
      "NerToken(word='鄭雲燦', ner='PERSON', idx=(181, 184))\n",
      "NerToken(word='三立新聞網', ner='ORG', idx=(271, 276))\n",
      "NerToken(word='歐漢聲', ner='PERSON', idx=(280, 283))\n",
      "NerToken(word='歐漢聲', ner='PERSON', idx=(326, 329))\n",
      "NerToken(word='鄭雲燦', ner='PERSON', idx=(330, 333))\n",
      "NerToken(word='鄭雲燦', ner='PERSON', idx=(346, 349))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623757\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.84s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.48s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.78s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.50s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.54s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.81s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 500.99it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.41s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.72s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='蘋果（Apple）iPhone 13系列', ner='PRODUCT', idx=(0, 20))\n",
      "NerToken(word='iPhone 1', ner='PRODUCT', idx=(57, 65))\n",
      "NerToken(word='iPhone 13 Pro', ner='PRODUCT', idx=(76, 89))\n",
      "NerToken(word='Pro Max', ner='ORG', idx=(90, 97))\n",
      "NerToken(word='中國', ner='GPE', idx=(107, 109))\n",
      "NerToken(word='Appleinsider', ner='ORG', idx=(123, 135))\n",
      "NerToken(word='美國', ner='GPE', idx=(136, 138))\n",
      "NerToken(word='摩根大通', ner='ORG', idx=(144, 148))\n",
      "NerToken(word='J.P.Morgan', ner='ORG', idx=(149, 159))\n",
      "NerToken(word='查特吉', ner='PERSON', idx=(163, 166))\n",
      "NerToken(word='Samik Chatterjee', ner='ORG', idx=(167, 183))\n",
      "NerToken(word='iPhone 13', ner='PRODUCT', idx=(192, 201))\n",
      "NerToken(word='iPhone 1', ner='PRODUCT', idx=(215, 223))\n",
      "NerToken(word='查特吉', ner='ORG', idx=(261, 264))\n",
      "NerToken(word='iPhone ', ner='GPE', idx=(281, 288))\n",
      "NerToken(word=' mini', ner='GPE', idx=(290, 295))\n",
      "NerToken(word='iPhone ', ner='GPE', idx=(296, 303))\n",
      "NerToken(word='iPhone 13 Pro', ner='ORG', idx=(329, 342))\n",
      "NerToken(word='Pro Max', ner='ORG', idx=(343, 350))\n",
      "NerToken(word='iPhone', ner='PRODUCT', idx=(389, 395))\n",
      "NerToken(word='美國', ner='GPE', idx=(403, 405))\n",
      "NerToken(word='iPhone', ner='ORG', idx=(475, 481))\n",
      "NerToken(word='中國', ner='GPE', idx=(489, 491))\n",
      "NerToken(word='iPhone ', ner='PERSON', idx=(492, 499))\n",
      "NerToken(word='iPhone 13 Pro', ner='PRODUCT', idx=(550, 563))\n",
      "NerToken(word='Pro Max', ner='PRODUCT', idx=(564, 571))\n",
      "NerToken(word='Appleinsider', ner='PERSON', idx=(592, 604))\n",
      "NerToken(word='查特吉', ner='PERSON', idx=(686, 689))\n",
      "NerToken(word='蘋果', ner='ORG', idx=(722, 724))\n",
      "NerToken(word='蘋果', ner='ORG', idx=(771, 773))\n",
      "NerToken(word='蘋果', ner='PERSON', idx=(810, 812))\n",
      "NerToken(word='iPhone 13系列', ner='PRODUCT', idx=(858, 869))\n",
      "NerToken(word='美系', ner='ORG', idx=(886, 888))\n",
      "NerToken(word='iPhone 13系列', ner='PRODUCT', idx=(906, 917))\n",
      "NerToken(word='iPhone 13', ner='PRODUCT', idx=(942, 951))\n",
      "NerToken(word='中央社', ner='ORG', idx=(967, 970))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623758\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='新北市板橋區', ner='LOC', idx=(7, 13))\n",
      "NerToken(word='陳', ner='PERSON', idx=(15, 16))\n",
      "NerToken(word='田單街', ner='LOC', idx=(26, 29))\n",
      "NerToken(word='陳男', ner='PERSON', idx=(60, 62))\n",
      "NerToken(word='陳', ner='PERSON', idx=(111, 112))\n",
      "NerToken(word='陳男', ner='PERSON', idx=(128, 130))\n",
      "NerToken(word='陳', ner='PERSON', idx=(189, 190))\n",
      "NerToken(word='田單街', ner='FAC', idx=(214, 217))\n",
      "NerToken(word='田單街', ner='FAC', idx=(247, 250))\n",
      "NerToken(word='許', ner='PERSON', idx=(257, 258))\n",
      "NerToken(word='陳男', ner='PERSON', idx=(283, 285))\n",
      "NerToken(word='陳男', ner='PERSON', idx=(307, 309))\n",
      "NerToken(word='陳男', ner='PERSON', idx=(393, 395))\n",
      "NerToken(word='許男', ner='PERSON', idx=(432, 434))\n",
      "NerToken(word='陳男', ner='PERSON', idx=(503, 505))\n",
      "NerToken(word='許男', ner='PERSON', idx=(553, 555))\n",
      "NerToken(word='TVBS', ner='WORK_OF_ART', idx=(561, 565))\n",
      "NerToken(word='法律扶助基金會', ner='ORG', idx=(602, 609))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623761\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.90it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='花蓮', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='中山路', ner='FAC', idx=(7, 10))\n",
      "NerToken(word='公正街包子店', ner='FAC', idx=(49, 55))\n",
      "NerToken(word='東大門', ner='FAC', idx=(85, 88))\n",
      "NerToken(word='明禮路', ner='FAC', idx=(89, 92))\n",
      "NerToken(word='花蓮', ner='GPE', idx=(156, 158))\n",
      "NerToken(word='花蓮', ner='GPE', idx=(278, 280))\n",
      "NerToken(word='中山路', ner='FAC', idx=(294, 297))\n",
      "NerToken(word='中山路段', ner='FAC', idx=(305, 309))\n",
      "NerToken(word='中華街', ner='FAC', idx=(309, 312))\n",
      "NerToken(word='中正路', ner='LOC', idx=(312, 315))\n",
      "NerToken(word='公正街公園路', ner='FAC', idx=(319, 325))\n",
      "NerToken(word='明智街', ner='FAC', idx=(325, 328))\n",
      "NerToken(word='公正街包子店', ner='FAC', idx=(459, 465))\n",
      "NerToken(word='中山路', ner='FAC', idx=(498, 501))\n",
      "NerToken(word='花蓮交通隊', ner='ORG', idx=(573, 578))\n",
      "NerToken(word='司俊華', ner='PERSON', idx=(580, 583))\n",
      "NerToken(word='中山公', ner='PERSON', idx=(585, 588))\n",
      "NerToken(word='明禮路', ner='LOC', idx=(667, 670))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623766\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.12s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.04s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.87s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='美國海軍', ner='ORG', idx=(0, 4))\n",
      "NerToken(word='南海', ner='LOC', idx=(20, 22))\n",
      "NerToken(word='美國第七艦隊', ner='ORG', idx=(42, 48))\n",
      "NerToken(word='康乃', ner='GPE', idx=(57, 59))\n",
      "NerToken(word='美國海軍學會新聞網', ner='ORG', idx=(76, 85))\n",
      "NerToken(word='第七艦隊', ner='ORG', idx=(99, 103))\n",
      "NerToken(word='希姆斯(Haley Sims)', ner='PERSON', idx=(106, 121))\n",
      "NerToken(word='印太地區', ner='LOC', idx=(139, 143))\n",
      "NerToken(word='南海', ner='LOC', idx=(181, 183))\n",
      "NerToken(word='關島', ner='GPE', idx=(185, 187))\n",
      "NerToken(word='美國海軍', ner='ORG', idx=(194, 198))\n",
      "NerToken(word='關島', ner='GPE', idx=(230, 232))\n",
      "NerToken(word='海軍海洋體系司令部', ner='ORG', idx=(234, 243))\n",
      "NerToken(word='Naval Sea Systems Command', ner='ORG', idx=(244, 269))\n",
      "NerToken(word='普及特海灣海軍造船廠', ner='ORG', idx=(271, 281))\n",
      "NerToken(word='關島', ner='GPE', idx=(373, 375))\n",
      "NerToken(word='關島', ner='GPE', idx=(396, 398))\n",
      "NerToken(word='第七艦隊', ner='ORG', idx=(435, 439))\n",
      "NerToken(word='湯馬斯', ner='PERSON', idx=(447, 450))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623769\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.94s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.12s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.93s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.92s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.85s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='蘋果公司', ner='ORG', idx=(35, 39))\n",
      "NerToken(word='911', ner='ORG', idx=(95, 98))\n",
      "NerToken(word='《華爾街日報》', ner='ORG', idx=(121, 128))\n",
      "NerToken(word='蘋果公司', ner='ORG', idx=(136, 140))\n",
      "NerToken(word='iOS 16', ner='PRODUCT', idx=(239, 245))\n",
      "NerToken(word='watchOS 9', ner='PRODUCT', idx=(246, 255))\n",
      "NerToken(word='蘋果公司', ner='ORG', idx=(286, 290))\n",
      "NerToken(word='蘋果', ner='ORG', idx=(300, 302))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623771\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.03it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='苗栗', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='竹南', ner='GPE', idx=(2, 4))\n",
      "NerToken(word='苗栗頭份', ner='GPE', idx=(181, 185))\n",
      "NerToken(word='竹南分局', ner='ORG', idx=(229, 233))\n",
      "NerToken(word='張中', ner='PERSON', idx=(237, 239))\n",
      "NerToken(word='苗栗南庄鄉', ner='GPE', idx=(396, 401))\n",
      "NerToken(word='新竹', ner='GPE', idx=(412, 414))\n",
      "NerToken(word='峨眉', ner='GPE', idx=(414, 416))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623773\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 64.00it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.30s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='塔利班', ner='ORG', idx=(0, 3))\n",
      "NerToken(word='阿富汗', ner='GPE', idx=(9, 12))\n",
      "NerToken(word='中國', ner='GPE', idx=(13, 15))\n",
      "NerToken(word='上海', ner='GPE', idx=(31, 33))\n",
      "NerToken(word='中國', ner='GPE', idx=(34, 36))\n",
      "NerToken(word='阿富汗', ner='GPE', idx=(37, 40))\n",
      "NerToken(word='王愚', ner='PERSON', idx=(42, 44))\n",
      "NerToken(word='塔利班', ner='ORG', idx=(45, 48))\n",
      "NerToken(word='喀布爾機場', ner='FAC', idx=(55, 60))\n",
      "NerToken(word='環球網', ner='ORG', idx=(76, 79))\n",
      "NerToken(word='美國之音', ner='ORG', idx=(80, 84))\n",
      "NerToken(word='VOA', ner='ORG', idx=(85, 88))\n",
      "NerToken(word='塔利班', ner='ORG', idx=(97, 100))\n",
      "NerToken(word='阿富汗', ner='GPE', idx=(106, 109))\n",
      "NerToken(word='中國', ner='GPE', idx=(112, 114))\n",
      "NerToken(word='阿富汗', ner='GPE', idx=(119, 122))\n",
      "NerToken(word='中國', ner='GPE', idx=(137, 139))\n",
      "NerToken(word='塔利班', ner='ORG', idx=(175, 178))\n",
      "NerToken(word='塔利班', ner='ORG', idx=(182, 185))\n",
      "NerToken(word='卡里米', ner='PERSON', idx=(190, 193))\n",
      "NerToken(word='喀布爾', ner='GPE', idx=(218, 221))\n",
      "NerToken(word='北京', ner='GPE', idx=(222, 224))\n",
      "NerToken(word='喀布爾', ner='GPE', idx=(287, 290))\n",
      "NerToken(word='阿富汗', ner='NORP', idx=(294, 297))\n",
      "NerToken(word='阿富汗', ner='GPE', idx=(310, 313))\n",
      "NerToken(word='哈納菲（Abdul Salam Hanafi', ner='PERSON', idx=(317, 339))\n",
      "NerToken(word='塔利班', ner='ORG', idx=(342, 345))\n",
      "NerToken(word='王愚', ner='PERSON', idx=(351, 353))\n",
      "NerToken(word='中國', ner='GPE', idx=(356, 358))\n",
      "NerToken(word='松子空中走廊', ner='LOC', idx=(370, 376))\n",
      "NerToken(word='卡里米', ner='PERSON', idx=(378, 381))\n",
      "NerToken(word='阿富汗', ner='GPE', idx=(386, 389))\n",
      "NerToken(word='阿富汗', ner='GPE', idx=(406, 409))\n",
      "NerToken(word='中國', ner='GPE', idx=(410, 412))\n",
      "NerToken(word='王愚', ner='PERSON', idx=(441, 443))\n",
      "NerToken(word='阿富汗', ner='GPE', idx=(450, 453))\n",
      "NerToken(word='中', ner='GPE', idx=(464, 465))\n",
      "NerToken(word='阿', ner='GPE', idx=(465, 466))\n",
      "NerToken(word='阿富汗', ner='NORP', idx=(501, 504))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NerToken(word='松子空中走廊', ner='FAC', idx=(512, 518))\n",
      "NerToken(word='中', ner='GPE', idx=(520, 521))\n",
      "NerToken(word='阿', ner='GPE', idx=(521, 522))\n",
      "NerToken(word='中國', ner='GPE', idx=(534, 536))\n",
      "NerToken(word='阿富汗', ner='GPE', idx=(537, 540))\n",
      "NerToken(word='塔利班', ner='ORG', idx=(551, 554))\n",
      "NerToken(word='阿富汗', ner='GPE', idx=(556, 559))\n",
      "NerToken(word='阿富汗', ner='NORP', idx=(563, 566))\n",
      "NerToken(word='中國', ner='GPE', idx=(578, 580))\n",
      "NerToken(word='松子空中走廊', ner='FAC', idx=(592, 598))\n",
      "NerToken(word='中國', ner='GPE', idx=(605, 607))\n",
      "NerToken(word='阿富汗', ner='GPE', idx=(609, 612))\n",
      "NerToken(word='中央社', ner='ORG', idx=(619, 622))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623786\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.74s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.12s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.27it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='吳泓逸', ner='PERSON', idx=(7, 10))\n",
      "NerToken(word='楊繡惠', ner='PERSON', idx=(115, 118))\n",
      "NerToken(word='吳宗憲', ner='PERSON', idx=(380, 383))\n",
      "NerToken(word='鳳梨', ner='NORP', idx=(389, 391))\n",
      "NerToken(word='楊繡惠', ner='PERSON', idx=(422, 425))\n",
      "NerToken(word='繡惠', ner='PERSON', idx=(433, 435))\n",
      "NerToken(word='鳳梨', ner='GPE', idx=(456, 458))\n",
      "NerToken(word='歐菈', ner='PERSON', idx=(461, 463))\n",
      "NerToken(word='小嫻', ner='PERSON', idx=(530, 532))\n",
      "NerToken(word='鳳梨', ner='GPE', idx=(603, 605))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623794\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.83s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.70s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='羅', ner='PERSON', idx=(0, 1))\n",
      "NerToken(word='高雄', ner='GPE', idx=(6, 8))\n",
      "NerToken(word='王', ner='PERSON', idx=(18, 19))\n",
      "NerToken(word='羅男', ner='PERSON', idx=(35, 37))\n",
      "NerToken(word='盧', ner='PERSON', idx=(49, 50))\n",
      "NerToken(word='羅', ner='PERSON', idx=(92, 93))\n",
      "NerToken(word='台北地方法院', ner='ORG', idx=(109, 115))\n",
      "NerToken(word='葉', ner='PERSON', idx=(138, 139))\n",
      "NerToken(word='雲林縣警察局', ner='ORG', idx=(149, 155))\n",
      "NerToken(word='新竹縣竹東鎮', ner='GPE', idx=(169, 175))\n",
      "NerToken(word='羅男', ner='PERSON', idx=(183, 185))\n",
      "NerToken(word='盧男', ner='PERSON', idx=(240, 242))\n",
      "NerToken(word='竹東', ner='GPE', idx=(248, 250))\n",
      "NerToken(word='新店溪', ner='LOC', idx=(332, 335))\n",
      "NerToken(word='竹東', ner='GPE', idx=(358, 360))\n",
      "NerToken(word='盧', ner='PERSON', idx=(402, 403))\n",
      "NerToken(word='羅男', ner='PERSON', idx=(409, 411))\n",
      "NerToken(word='羅男', ner='PERSON', idx=(441, 443))\n",
      "NerToken(word='兒童及少年性剝削防制條例', ner='LAW', idx=(449, 461))\n",
      "NerToken(word='羅', ner='PERSON', idx=(478, 479))\n",
      "NerToken(word='葉', ner='PERSON', idx=(489, 490))\n",
      "NerToken(word='王男依', ner='PERSON', idx=(540, 543))\n",
      "NerToken(word='盧男', ner='PERSON', idx=(560, 562))\n",
      "NerToken(word='高雄高分院', ner='ORG', idx=(571, 576))\n",
      "NerToken(word='新店溪', ner='LOC', idx=(634, 637))\n",
      "NerToken(word='中央社', ner='ORG', idx=(718, 721))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623796\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='西班亞', ner='GPE', idx=(2, 5))\n",
      "NerToken(word='瓦倫西亞', ner='GPE', idx=(5, 9))\n",
      "NerToken(word='翁達市', ner='GPE', idx=(10, 13))\n",
      "NerToken(word='西班牙', ner='GPE', idx=(133, 136))\n",
      "NerToken(word='西班牙', ner='GPE', idx=(459, 462))\n",
      "NerToken(word='奔牛節', ner='EVENT', idx=(473, 476))\n",
      "NerToken(word='西班牙', ner='GPE', idx=(541, 544))\n",
      "NerToken(word='潘普洛納', ner='GPE', idx=(573, 577))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623804\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 946.80it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.71s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.41it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.72s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.70s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.58s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='奈及利亞', ner='GPE', idx=(0, 4))\n",
      "NerToken(word='拉哥斯（Lagos）', ner='FAC', idx=(8, 18))\n",
      "NerToken(word='拉哥斯', ner='GPE', idx=(57, 60))\n",
      "NerToken(word='奈及利亞', ner='GPE', idx=(61, 65))\n",
      "NerToken(word='拉哥斯', ner='GPE', idx=(103, 106))\n",
      "NerToken(word='法新社', ner='ORG', idx=(123, 126))\n",
      "NerToken(word='路透社', ner='ORG', idx=(154, 157))\n",
      "NerToken(word='奈及利亞', ner='GPE', idx=(228, 232))\n",
      "NerToken(word='國家緊急事故管理署', ner='ORG', idx=(232, 241))\n",
      "NerToken(word='NEMA', ner='ORG', idx=(242, 246))\n",
      "NerToken(word='費林羅伊', ner='PERSON', idx=(249, 253))\n",
      "NerToken(word='拉哥斯州', ner='GPE', idx=(290, 294))\n",
      "NerToken(word='森武-奧盧', ner='PERSON', idx=(296, 301))\n",
      "NerToken(word='約翰（Wisdom John）', ner='PERSON', idx=(354, 369))\n",
      "NerToken(word='約翰', ner='PERSON', idx=(411, 413))\n",
      "NerToken(word='路透社', ner='ORG', idx=(508, 511))\n",
      "NerToken(word='拉哥斯州', ner='GPE', idx=(613, 617))\n",
      "NerToken(word='森武-奧盧', ner='PERSON', idx=(637, 642))\n",
      "NerToken(word='奈及利亞', ner='GPE', idx=(657, 661))\n",
      "NerToken(word='非洲', ner='LOC', idx=(662, 664))\n",
      "NerToken(word='拉哥斯', ner='GPE', idx=(669, 672))\n",
      "NerToken(word='拉哥斯', ner='GPE', idx=(715, 718))\n",
      "NerToken(word='南非人', ner='NORP', idx=(740, 743))\n",
      "NerToken(word='奈及利亞', ner='GPE', idx=(787, 791))\n",
      "NerToken(word='烏約市（Uyo）', ner='FAC', idx=(794, 802))\n",
      "NerToken(word='中央社', ner='ORG', idx=(822, 825))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623816\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.27s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='高雄仁武', ner='GPE', idx=(0, 4))\n",
      "NerToken(word='仁武警', ner='ORG', idx=(594, 597))\n",
      "NerToken(word='徐如姣', ner='PERSON', idx=(601, 604))\n",
      "NerToken(word='仁武', ner='GPE', idx=(630, 632))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623820\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.63s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 500.99it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.62s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.17it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.67s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.23it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.61s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.35it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.77s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 501.05it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:02<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='印度', ner='GPE', idx=(0, 2))\n",
      "NerToken(word='莫迪', ner='PERSON', idx=(4, 6))\n",
      "NerToken(word='英國', ner='GPE', idx=(9, 11))\n",
      "NerToken(word='格拉斯哥', ner='GPE', idx=(11, 15))\n",
      "NerToken(word='印度', ner='GPE', idx=(23, 25))\n",
      "NerToken(word='聯合國氣候變化綱要公約第26次締約方會議', ner='EVENT', idx=(71, 91))\n",
      "NerToken(word='法新社', ner='ORG', idx=(141, 144))\n",
      "NerToken(word='莫迪（Narendra Modi', ner='PERSON', idx=(147, 163))\n",
      "NerToken(word='印度', ner='GPE', idx=(190, 192))\n",
      "NerToken(word='印度', ner='GPE', idx=(246, 248))\n",
      "NerToken(word='中國', ner='GPE', idx=(263, 265))\n",
      "NerToken(word='美國', ner='GPE', idx=(277, 279))\n",
      "NerToken(word='歐洲', ner='LOC', idx=(280, 282))\n",
      "NerToken(word='莫迪', ner='PERSON', idx=(294, 296))\n",
      "NerToken(word='印度', ner='GPE', idx=(299, 301))\n",
      "NerToken(word='莫迪', ner='PERSON', idx=(341, 343))\n",
      "NerToken(word='路透社', ner='ORG', idx=(358, 361))\n",
      "NerToken(word='莫迪', ner='PERSON', idx=(364, 366))\n",
      "NerToken(word='印度', ner='GPE', idx=(375, 377))\n",
      "NerToken(word='印度', ner='GPE', idx=(412, 414))\n",
      "NerToken(word='印度', ner='GPE', idx=(458, 460))\n",
      "NerToken(word='印度', ner='GPE', idx=(489, 491))\n",
      "NerToken(word='莫迪', ner='PERSON', idx=(514, 516))\n",
      "NerToken(word='哥本哈根COP15氣候峰會', ner='EVENT', idx=(566, 579))\n",
      "NerToken(word='COVID-19', ner='EVENT', idx=(657, 665))\n",
      "NerToken(word='巴黎', ner='GPE', idx=(681, 683))\n",
      "NerToken(word='富國', ner='ORG', idx=(691, 693))\n",
      "NerToken(word='莫迪', ner='PERSON', idx=(710, 712))\n",
      "NerToken(word='印度', ner='GPE', idx=(747, 749))\n",
      "NerToken(word='莫迪', ner='PERSON', idx=(770, 772))\n",
      "NerToken(word='印度', ner='GPE', idx=(803, 805))\n",
      "NerToken(word='莫迪', ner='PERSON', idx=(816, 818))\n",
      "NerToken(word='富國', ner='ORG', idx=(826, 828))\n",
      "NerToken(word='印度', ner='GPE', idx=(851, 853))\n",
      "NerToken(word='莫迪', ner='PERSON', idx=(876, 878))\n",
      "NerToken(word='富國', ner='GPE', idx=(888, 890))\n",
      "NerToken(word='印度', ner='GPE', idx=(906, 908))\n",
      "NerToken(word='越南', ner='GPE', idx=(911, 913))\n",
      "NerToken(word='范明正', ner='PERSON', idx=(915, 918))\n",
      "NerToken(word='越南', ner='GPE', idx=(922, 924))\n",
      "NerToken(word='路透社', ner='ORG', idx=(940, 943))"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NerToken(word='美', ner='GPE', idx=(946, 947))\n",
      "NerToken(word='英', ner='GPE', idx=(948, 949))\n",
      "NerToken(word='歐洲聯盟', ner='ORG', idx=(950, 954))\n",
      "NerToken(word='EU', ner='ORG', idx=(955, 957))\n",
      "NerToken(word='中央社', ner='ORG', idx=(1053, 1056))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623824\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.46it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.82s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.80s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.42it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.76s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1000.79it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='高雄', ner='GPE', idx=(12, 14))\n",
      "NerToken(word='檢疫旅館', ner='FAC', idx=(37, 41))\n",
      "NerToken(word='傳染病防治法', ner='LAW', idx=(72, 78))\n",
      "NerToken(word='許', ner='PERSON', idx=(99, 100))\n",
      "NerToken(word='高雄鹽埕區', ner='LOC', idx=(108, 113))\n",
      "NerToken(word='防火巷', ner='FAC', idx=(180, 183))\n",
      "NerToken(word='傳染病防治法', ner='LAW', idx=(327, 333))\n",
      "NerToken(word='衛生局', ner='ORG', idx=(344, 347))\n",
      "NerToken(word='鹽埕警分局長', ner='ORG', idx=(368, 374))\n",
      "NerToken(word='蔡鴻文', ner='PERSON', idx=(374, 377))\n",
      "NerToken(word='衛生局', ner='ORG', idx=(411, 414))\n",
      "NerToken(word='TVBS', ner='WORK_OF_ART', idx=(436, 440))\n",
      "NerToken(word='新冠肺炎', ner='EVENT', idx=(447, 451))\n",
      "NerToken(word='疾管署', ner='ORG', idx=(454, 457))\n",
      "NerToken(word='\\xa00800-001922', ner='PRODUCT', idx=(490, 502))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623837\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.58s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.65s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.98it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.57s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1003.18it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 500.99it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.49s/it]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='台北市', ner='GPE', idx=(0, 3))\n",
      "NerToken(word='林', ner='PERSON', idx=(9, 10))\n",
      "NerToken(word='松山區市民大道六段', ner='FAC', idx=(34, 43))\n",
      "NerToken(word='林男', ner='PERSON', idx=(85, 87))\n",
      "NerToken(word='林男', ner='PERSON', idx=(113, 115))\n",
      "NerToken(word='萬華區', ner='LOC', idx=(178, 181))\n",
      "NerToken(word='林', ner='PERSON', idx=(230, 231))\n",
      "NerToken(word='市民大道六段', ner='FAC', idx=(279, 285))\n",
      "NerToken(word='林男', ner='PERSON', idx=(294, 296))\n",
      "NerToken(word='郭', ner='PERSON', idx=(337, 338))\n",
      "NerToken(word='蕭', ner='PERSON', idx=(346, 347))\n",
      "NerToken(word='婦奔警局', ner='ORG', idx=(387, 391))\n",
      "NerToken(word='郭男', ner='PERSON', idx=(402, 404))\n",
      "NerToken(word='林男', ner='PERSON', idx=(408, 410))\n",
      "NerToken(word='林男', ner='PERSON', idx=(419, 421))\n",
      "NerToken(word='林男', ner='PERSON', idx=(458, 460))\n",
      "NerToken(word='郭男', ner='PERSON', idx=(482, 484))\n",
      "NerToken(word='林男', ner='PERSON', idx=(499, 501))\n",
      "NerToken(word='林', ner='LOC', idx=(572, 573))\n",
      "NerToken(word='萬華區', ner='LOC', idx=(600, 603))\n",
      "NerToken(word='蕭男', ner='PERSON', idx=(671, 673))\n",
      "NerToken(word='郭', ner='PERSON', idx=(676, 677))\n",
      "NerToken(word='少年法庭', ner='ORG', idx=(708, 712))\n",
      "NerToken(word='TVBS', ner='WORK_OF_ART', idx=(716, 720))\n",
      "NerToken(word='法律扶助基金會', ner='ORG', idx=(757, 764))\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "1623876\n",
      "TVBS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.78it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.51it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.22it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.25it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1002.70it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.51it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  2.58it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<00:00, 1001.74it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.20it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  3.95it/s]\n",
      "\n",
      "Tokenization: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NerToken(word='新北市', ner='GPE', idx=(6, 9))\n",
      "NerToken(word='三重', ner='GPE', idx=(9, 11))\n",
      "NerToken(word='蘆洲', ner='LOC', idx=(53, 55))\n",
      "NerToken(word='立法院長', ner='ORG', idx=(82, 86))\n",
      "NerToken(word='王金平', ner='PERSON', idx=(86, 89))\n",
      "NerToken(word='王金平', ner='PERSON', idx=(99, 102))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-a66e2c9f30d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_of_dicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mNews_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist_of_dicts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'News_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "r = requests.get(\"http://140.134.26.31:3000/api/process/getNews/20211027?$format=json\", verify=False)\n",
    "list_of_dicts = r.json()\n",
    "print(type(r))\n",
    "print(type(list_of_dicts))\n",
    "\n",
    "\n",
    "for i in range(0,10000):\n",
    "    \n",
    "    text = list_of_dicts[\"data\"][i]['Content']\n",
    "    text = [str(text)]\n",
    "    News_id = list_of_dicts[\"data\"][i]['News_id']\n",
    "    Provenance = list_of_dicts[\"data\"][i]['Provenance']\n",
    "    Provenance = str(Provenance)\n",
    "    print(type(News_id))\n",
    "    print(type(Provenance))\n",
    "    print(News_id)\n",
    "    print(Provenance)\n",
    "\n",
    "    \n",
    "    # Run pipeline\n",
    "    ws  = ws_driver(text)\n",
    "    pos = pos_driver(ws)\n",
    "    ner = ner_driver(text)\n",
    "\n",
    "    # Enable sentence segmentation\n",
    "    ws  = ws_driver(text, use_delim=True)\n",
    "    ner = ner_driver(text, use_delim=True)\n",
    "\n",
    "    # Disable sentence segmentation\n",
    "    pos = pos_driver(ws, use_delim=False)\n",
    "\n",
    "    # Use new line characters and tabs for sentence segmentation\n",
    "    pos = pos_driver(ws, delim_set='\\n\\t')\n",
    "\n",
    "    # Sets the batch size and maximum sentence length\n",
    "    ws = ws_driver(text, batch_size=256, max_length=500)\n",
    "\n",
    "    # Pack word segmentation and part-of-speech results\n",
    "    def pack_ws_pos_sentece(sentence_ws, sentence_pos):\n",
    "        assert len(sentence_ws) == len(sentence_pos)\n",
    "        res = []\n",
    "        for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
    "            res.append(f'{word_ws}({word_pos})')\n",
    "        return '\\u3000'.join(res)\n",
    "\n",
    "    # Show results\n",
    "    for sentence, sentence_ws, sentence_pos, sentence_ner in zip(text, ws, pos, ner):\n",
    "       # print(sentence)\n",
    "       # print(pack_ws_pos_sentece(sentence_ws, sentence_pos))\n",
    "\n",
    "        for entity in sentence_ner:\n",
    "            if (entity[1] == 'DATE' or entity[1] == 'TIME' or entity[1] =='CARDINAL' or \n",
    "                entity[1] =='MONEY' or entity[1] =='PERCENT' or entity[1] =='ORDINAL'or entity[1] =='QUANTITY'): \n",
    "                continue\n",
    "            Keyword= str(entity[0])\n",
    "            api_url = \"http://140.134.26.31:3000/api/keyword/postKeyword\"\n",
    "            todo = {'newsID':News_id,'provenance':Provenance,'keyword':Keyword}\n",
    "            response = requests.post(api_url, json=todo)\n",
    "            response.json()\n",
    "            print(entity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3c7a819655e9f6c16ec335f8a3abf936bf261f9d6cb538ae0c27bb7ccda00602"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('Bert_EDModel': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
